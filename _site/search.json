[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wong Kelly",
    "section": "",
    "text": "Welcome to my IS415 Geospatial Analytics and Applications Webpage! :)\n Fun Fact: I LOVE MIFFY\nHello! Let me officially introduce myself! My name is Miffy and this is the course website of IS415 I study this term. You will find my course work publish on this website."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex02.html",
    "href": "In_Class_Ex/In_Class_Ex02.html",
    "title": "In Class Exercise 2: Geospatial Data Wrangling",
    "section": "",
    "text": "Water is an important resource to mankind. Clean and accessible water is critical to human health. It provides a healthy environment, a sustainable economy, reduces poverty and ensures peace and security. Yet over 40% of the global population does not have access to sufficient clean water. By 2025, 1.8 billion people will be living in countries or regions with absolute water scarcity, according to UN-Water. The lack of water poses a major threat to several sectors, including food security. Agriculture uses about 70% of the world’s accessible freshwater.\nDeveloping countries are most affected by water shortages and poor water quality. Up to 80% of illnesses in the developing world are linked to inadequate water and sanitation. Despite technological advancement, providing clean water to the rural community is still a major development issues in many countries globally, especially countries in the Africa continent.\nTo address the issue of providing clean and sustainable water supply to the rural community, a global Water Point Data Exchange (WPdx) project has been initiated. The main aim of this initiative is to collect water point related data from rural areas at the water point or small water scheme level and share the data via WPdx Data Repository, a cloud-based data library. What is so special of this project is that data are collected based on WPDx Data Standard.\n\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate geospatial data wrangling methods to prepare the data for water point mapping study. For the purpose of this study, Nigeria will be used as the study country.\n\n\n\n\n\nFor the purpose of this assignment, data from WPdx Global Data Repositories will be used. There are two versions of the data. They are: WPdx-Basic and WPdx+. You are required to use WPdx+ data set.\n\n\n\nNigeria Level-2 Administrative Boundary (also known as Local Government Area) polygon features GIS data will be used in this take-home exercise. The data can be downloaded either from The Humanitarian Data Exchange portal or geoBoundaries.\n\n\n\n\nThe specific tasks of this take-home exercise are as follows:\n\nUsing appropriate sf method, import the shapefile into R and save it in a simple feature data frame format. Note that there are three Projected Coordinate Systems of Nigeria, they are: EPSG: 26391, 26392, and 26303. You can use any one of them.\nUsing appropriate tidyr and dplyr methods, derive the number of functional and non-functional water points at LGA level.\nCombining the geospatial and aspatial data frame into simple feature data frame.\nVisualising the distribution of water point by using appropriate statistical methods."
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex02.html#getting-started",
    "href": "In_Class_Ex/In_Class_Ex02.html#getting-started",
    "title": "In Class Exercise 2: Geospatial Data Wrangling",
    "section": "2. Getting started",
    "text": "2. Getting started\nFor the purpose of this in-class exercise, three R packages will be used. They are: sf, tidyverse and funModeling.\n\npacman::p_load(sf, tidyverse, funModeling)"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex02.html#handling-geospatial-data",
    "href": "In_Class_Ex/In_Class_Ex02.html#handling-geospatial-data",
    "title": "In Class Exercise 2: Geospatial Data Wrangling",
    "section": "3. Handling Geospatial Data",
    "text": "3. Handling Geospatial Data\n\n3.1 Importing Geospatial\n\n3.1.1 The geoBoundaries data set\n\ngeoNGA <- st_read(\"data/geospatial/geoBoundaries-NGA-ADM2-all/\",\n                  layer = \"geoBoundaries-NGA-ADM2\") %>%\n  st_transform(crs = 26392)\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `C:\\G2wongkelly\\IS415_GAA\\In_Class_Ex\\data\\geospatial\\geoBoundaries-NGA-ADM2-all' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\n\n\n3.1.2 The NGA data set\n\nNGA <- st_read(\"data/geospatial/nga_adm_osgof_20190417/\",\n               layer = \"geoBoundaries-NGA-ADM2\") %>%\n  st_transform(crs = 26392)\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `C:\\G2wongkelly\\IS415_GAA\\In_Class_Ex\\data\\geospatial\\nga_adm_osgof_20190417' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\nBy examining both sf dataframe closely, we notice that NGA provide both LGA and state information. Hence, NGA data.frame will be used for the subsequent processing.\n\n\n\n3.2 Importing Aspatial Data\n\nwp_nga <- read_csv(\"data/aspatial/WPdx.csv\") %>%\n  filter(`#clean_country_name` == \"Nigeria\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n\n\nRows: 406566 Columns: 70\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (43): #source, #report_date, #status_id, #water_source_clean, #water_sou...\ndbl (23): row_id, #lat_deg, #lon_deg, #install_year, #fecal_coliform_value, ...\nlgl  (4): #rehab_year, #rehabilitator, is_urban, latest_record\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n3.2.1 Converting water point data into sf point features\nConverting an aspatial data into an sf data.frame involves two steps.\nFirst, we need to convert the wkt field into sfc field by using st_as_sfc() data type.\n\nwp_nga$Geometry = st_as_sfc(wp_nga$`New Georeferenced Column`)\nwp_nga\n\n# A tibble: 95,008 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n    <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\nNext, we will convert the tibble data.frame into an sf object by using st_sf(). It is also important for us to include the referencing system of the data into the sf object.\n\nwp_sf <- st_sf(wp_nga, crs=4326)\nwp_sf\n\nSimple feature collection with 95008 features and 70 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2.707441 ymin: 4.301812 xmax: 14.21828 ymax: 13.86568\nGeodetic CRS:  WGS 84\n# A tibble: 95,008 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n *  <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\n\n\n3.2.2 Transforming into Nigeria projected coordinated system\n\nwp_sf <- wp_sf %>%\n  st_transform(crs = 26392)"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex02.html#geospatial-data-cleaning",
    "href": "In_Class_Ex/In_Class_Ex02.html#geospatial-data-cleaning",
    "title": "In Class Exercise 2: Geospatial Data Wrangling",
    "section": "4. Geospatial Data Cleaning",
    "text": "4. Geospatial Data Cleaning\nData cleaning is an important step in any data science task including geospatial data science. It is important for us to do our due deligent to check if any data quality issues occured in the data used."
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex02.html#excluding-redundent-fields",
    "href": "In_Class_Ex/In_Class_Ex02.html#excluding-redundent-fields",
    "title": "In Class Exercise 2: Geospatial Data Wrangling",
    "section": "4.1 Excluding redundent fields",
    "text": "4.1 Excluding redundent fields\nNGA sf data.frame consists of many redundent fields. The code chunk below uses select() of dplyr to retain column 3, 4, 8 and 9. Do you know why?"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html",
    "href": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "",
    "text": "Water is an important resource to mankind. Clean and accessible water is critical to human health. It provides a healthy environment, a sustainable economy, reduces poverty and ensures peace and security. Yet over 40% of the global population does not have access to sufficient clean water. By 2025, 1.8 billion people will be living in countries or regions with absolute water scarcity, according to UN-Water. The lack of water poses a major threat to several sectors, including food security. Agriculture uses about 70% of the world’s accessible freshwater.\nDeveloping countries are most affected by water shortages and poor water quality. Up to 80% of illnesses in the developing world are linked to inadequate water and sanitation. Despite technological advancement, providing clean water to the rural community is still a major development issues in many countries globally, especially countries in the Africa continent.\nTo address the issue of providing clean and sustainable water supply to the rural community, a global Water Point Data Exchange (WPdx) project has been initiated. The main aim of this initiative is to collect water point related data from rural areas at the water point or small water scheme level and share the data via WPdx Data Repository, a cloud-based data library. What is so special of this project is that data are collected based on WPDx Data Standard.\n\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate geospatial data wrangling methods to prepare the data for water point mapping study. For the purpose of this study, Nigeria will be used as the study country.\n\n\n\n\n\nFor the purpose of this assignment, data from WPdx Global Data Repositories will be used. There are two versions of the data. They are: WPdx-Basic and WPdx+. You are required to use WPdx+ data set.\n\n\n\nNigeria Level-2 Administrative Boundary (also known as Local Government Area) polygon features GIS data will be used in this take-home exercise. The data can be downloaded either from The Humanitarian Data Exchange portal or geoBoundaries.\n\n\n\n\nThe specific tasks of this take-home exercise are as follows:\n\nUsing appropriate sf method, import the shapefile into R and save it in a simple feature data frame format. Note that there are three Projected Coordinate Systems of Nigeria, they are: EPSG: 26391, 26392, and 26303. You can use any one of them.\nUsing appropriate tidyr and dplyr methods, derive the number of functional and non-functional water points at LGA level.\nCombining the geospatial and aspatial data frame into simple feature data frame.\nVisualising the distribution of water point by using appropriate statistical methods."
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#getting-started",
    "href": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#getting-started",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "2 Getting started",
    "text": "2 Getting started\nFor the purpose of this in-class exercise, three R packages will be used. They are: sf, tidyverse and funModeling.\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the step you had learned, check if these three R packages have been installed in you laptop, if not install the missing R packages. If Yes, launch the R packages into R environment\n\n\n\n\n\nShow the code\npacman::p_load(sf, tidyverse, funModeling)"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#handling-geospatial-data",
    "href": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#handling-geospatial-data",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "3 Handling Geospatial Data",
    "text": "3 Handling Geospatial Data\n\n3.1 Importing Geospatial\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the step you had learned, import the LGA boundary GIS data of Nigeria downloaded from both sources recommend above.\n\n\n\n3.1.1 The geoBoundaries data set\n\n\n\nShow the code\ngeoNGA <- st_read(\"data/geospatial/\",\n                  layer = \"geoBoundaries-NGA-ADM2\") %>%\n  st_transform(crs = 26392)\n\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `C:\\G2wongkelly\\IS415_GAA\\In_Class_Ex\\In_Class_Ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\n\n\n\n3.1.2 The NGA data set\n\n\n\nShow the code\nNGA <- st_read(\"data/geospatial/\",\n               layer = \"nga_admbnda_adm2_osgof_20190417\") %>%\n  st_transform(crs = 26392)\n\n\nReading layer `nga_admbnda_adm2_osgof_20190417' from data source \n  `C:\\G2wongkelly\\IS415_GAA\\In_Class_Ex\\In_Class_Ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\n\nBy examining both sf dataframe closely, we notice that NGA provide both LGA and state information. Hence, NGA data.frame will be used for the subsequent processing.\n\n\n\n3.2 Importing Aspatial data\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the steps you had learned, import the downloaded water point data set into R, at the same time select only water points within Nigeria.\n\n\n\n\n\nShow the code\nwp_nga <- read_csv(\"data/aspatial/WPdx.csv\") %>%\n  filter(`#clean_country_name` == \"Nigeria\")\n\n\n\n\n3.2.1 Converting water point data into sf point features\nConverting an aspatial data into an sf data.frame involves two steps.\nFirst, we need to convert the wkt field into sfc field by using st_as_sfc() data type.\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the steps you had learned, convert the newly extracted wp_NGA into an sf data.frame\n\n\n\n\n\nShow the code\nwp_nga$Geometry = st_as_sfc(wp_nga$`New Georeferenced Column`)\nwp_nga\n\n\n# A tibble: 95,008 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n    <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\n\nNext, we will convert the tibble data.frame into an sf object by using st_sf(). It is also important for us to include the referencing system of the data into the sf object.\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the steps you had learned, convert the newly extracted wp_NGA into an sf data.frame\n\n\n\n\n\nShow the code\nwp_sf <- st_sf(wp_nga, crs=4326)\nwp_sf\n\n\nSimple feature collection with 95008 features and 70 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 2.707441 ymin: 4.301812 xmax: 14.21828 ymax: 13.86568\nGeodetic CRS:  WGS 84\n# A tibble: 95,008 × 71\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n *  <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 429068 GRID3             7.98    5.12 08/29/… Unknown <NA>    <NA>    Tapsta…\n 2 222071 Federal Minis…    6.96    3.60 08/16/… Yes     Boreho… Well    Mechan…\n 3 160612 WaterAid          6.49    7.93 12/04/… Yes     Boreho… Well    Hand P…\n 4 160669 WaterAid          6.73    7.65 12/04/… Yes     Boreho… Well    <NA>   \n 5 160642 WaterAid          6.78    7.66 12/04/… Yes     Boreho… Well    Hand P…\n 6 160628 WaterAid          6.96    7.78 12/04/… Yes     Boreho… Well    Hand P…\n 7 160632 WaterAid          7.02    7.84 12/04/… Yes     Boreho… Well    Hand P…\n 8 642747 Living Water …    7.33    8.98 10/03/… Yes     Boreho… Well    Mechan…\n 9 642456 Living Water …    7.17    9.11 10/03/… Yes     Boreho… Well    Hand P…\n10 641347 Living Water …    7.20    9.22 03/28/… Yes     Boreho… Well    Hand P…\n# … with 94,998 more rows, 62 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\n\n\n\n3.2.2 Transforming into Nigeria projected coordinate system\n\n\n\n\n\n\nYour turn\n\n\n\nUsing the steps you had learned, transform the projection from wgs84 to appropriate projected coordinate system of Nigeria.\n\n\n\n\n\nShow the code\nwp_sf <- wp_sf %>%\n  st_transform(crs = 26392)"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#geospatial-data-cleaning",
    "href": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#geospatial-data-cleaning",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "4 Geospatial Data Cleaning",
    "text": "4 Geospatial Data Cleaning\nData cleaning is an important step in any data science task including geospatial data science. It is important for us to do our due deligent to check if any data quality issues occured in the data used.\n\n4.1 Excluding redundent fields\nNGA sf data.frame consists of many redundent fields. The code chunk below uses select() of dplyr to retain column 3, 4, 8 and 9. Do you know why?\n\n\nNGA <- NGA %>%\n  select(c(3:4, 8:9))\n\n\n\n\n4.2 Checking for duplicate name\nIt is always important to check for duplicate name in the data main data fields. Using duplicated() of Base R, we can flag out LGA names that might be duplicated as shown in the code chunk below.\n\n\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\n[1] \"Bassa\"    \"Ifelodun\" \"Irepodun\" \"Nasarawa\" \"Obi\"      \"Surulere\"\n\n\n\nThe printout above shows that there are 6 LGAs with the same name. A Google search using the coordinates showed that there are LGAs with the same name but are located in different states. For instances, there is a Bassa LGA in Kogi State and a Bassa LGA in Plateau State.\nLet us correct these errors by using the code chunk below.\n\n\nNGA$ADM2_EN[94] <- \"Bassa, Kogi\"\nNGA$ADM2_EN[95] <- \"Bassa, Plateau\"\nNGA$ADM2_EN[304] <- \"Ifelodun, Kwara\"\nNGA$ADM2_EN[305] <- \"Ifelodun, Osun\"\nNGA$ADM2_EN[355] <- \"Irepodun, Kwara\"\nNGA$ADM2_EN[356] <- \"Irepodun, Osun\"\nNGA$ADM2_EN[519] <- \"Nasarawa, Kano\"\nNGA$ADM2_EN[520] <- \"Nasarawa, Nasarawa\"\nNGA$ADM2_EN[546] <- \"Obi, Benue\"\nNGA$ADM2_EN[547] <- \"Obi, Nasarawa\"\nNGA$ADM2_EN[693] <- \"Surulere, Lagos\"\nNGA$ADM2_EN[694] <- \"Surulere, Oyo\"\n\n\nNow, let us rerun the code chunk below to confirm that the duplicated name issue has been addressed.\n\n\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\ncharacter(0)"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#excluding-redundent-fields",
    "href": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#excluding-redundent-fields",
    "title": "In Class Exercise 2: Geospatial Data Wrangling",
    "section": "4.1 Excluding redundent fields",
    "text": "4.1 Excluding redundent fields\nNGA sf data.frame consists of many redundent fields. The code chunk below uses select() of dplyr to retain column 3, 4, 8 and 9. Do you know why?\n\nNGA <- NGA %>%\n  dplyr::select(3:4, 8:9)"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#checking-for-duplicate-name",
    "href": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#checking-for-duplicate-name",
    "title": "In Class Exercise 2: Geospatial Data Wrangling",
    "section": "4.2 Checking for duplicate name",
    "text": "4.2 Checking for duplicate name\n\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\n[1] \"Bassa\"    \"Ifelodun\" \"Irepodun\" \"Nasarawa\" \"Obi\"      \"Surulere\"\n\n\nThe printout above shows that there are 6 LGAs with the same name. A Google search using the coordinates showed that there are LGAs with the same name but are located in different states. For instances, there is a Bassa LGA in Kogi State and a Bassa LGA in Plateau State.\nLet us correct these errors by using the code chunk below.\n\nNGA$ADM2_EN[94] <- \"Bassa, Kogi\"\nNGA$ADM2_EN[95] <- \"Bassa, Plateau\"\nNGA$ADM2_EN[304] <- \"Ifelodun, Kwara\"\nNGA$ADM2_EN[305] <- \"Ifelodun, Osun\"\nNGA$ADM2_EN[355] <- \"Irepodun, Kwara\"\nNGA$ADM2_EN[356] <- \"Irepodun, Osun\"\nNGA$ADM2_EN[519] <- \"Nasarawa, Kano\"\nNGA$ADM2_EN[520] <- \"Nasarawa, Nasarawa\"\nNGA$ADM2_EN[546] <- \"Obi, Benue\"\nNGA$ADM2_EN[547] <- \"Obi, Nasarawa\"\nNGA$ADM2_EN[693] <- \"Surulere, Lagos\"\nNGA$ADM2_EN[694] <- \"Surulere, Oyo\"\n\nNow, let us rerun the code chunk below to confirm that the duplicated name issue has been addressed.\n\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\ncharacter(0)"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#data-wrangling-for-water-point-data",
    "href": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#data-wrangling-for-water-point-data",
    "title": "In-class Exercise 2: Geospatial Data Wrangling",
    "section": "5 Data Wrangling for Water Point Data",
    "text": "5 Data Wrangling for Water Point Data\nExploratory Data Analysis (EDA) is a popular approach to gain initial understanding of the data. In the code chunk below, freq() of funModeling package is used to reveal the distribution of water point status visually.\n\n\nfreq(data = wp_sf,\n     input = '#status_clean')\n\n\n\n\n                     #status_clean frequency percentage cumulative_perc\n1                       Functional     45883      48.29           48.29\n2                   Non-Functional     29385      30.93           79.22\n3                             <NA>     10656      11.22           90.44\n4      Functional but needs repair      4579       4.82           95.26\n5 Non-Functional due to dry season      2403       2.53           97.79\n6        Functional but not in use      1686       1.77           99.56\n7         Abandoned/Decommissioned       234       0.25           99.81\n8                        Abandoned       175       0.18           99.99\n9 Non functional due to dry season         7       0.01          100.00\n\n\n\nFigure above shows that there are nine classes in the #status_clean fields.\nNext, code chunk below will be used to perform the following data wrangling tasksP - rename() of dplyr package is used to rename the column from #status_clean to status_clean for easier handling in subsequent steps. - select() of dplyr is used to include status_clean in the output sf data.frame. - mutate() and replace_na() are used to recode all the NA values in status_clean into unknown.\n\n\nwp_sf_nga <- wp_sf %>% \n  rename(status_clean = '#status_clean') %>%\n  select(status_clean) %>%\n  mutate(status_clean = replace_na(\n    status_clean, \"unknown\"))\n\n\n\n5.1 Extracting Water Point Data\nNow we are ready to extract the water point data according to their status.\nThe code chunk below is used to extract functional water point.\n\n\nwp_functional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\"Functional\",\n             \"Functional but not in use\",\n             \"Functional but needs repair\"))\n\n\nThe code chunk below is used to extract nonfunctional water point.\n\n\nwp_nonfunctional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\"Abandoned/Decommissioned\",\n             \"Abandoned\",\n             \"Non-Functional due to dry season\",\n             \"Non-Functional\",\n             \"Non functional due to dry season\"))\n\n\nThe code chunk below is used to extract water point with unknown status.\n\n\nwp_unknown <- wp_sf_nga %>%\n  filter(status_clean == \"unknown\")\n\n\nNext, the code chunk below is used to perform a quick EDA on the derived sf data.frames.\n\n\nfreq(data = wp_functional,\n     input = 'status_clean')\n\n\n\n\n                 status_clean frequency percentage cumulative_perc\n1                  Functional     45883      87.99           87.99\n2 Functional but needs repair      4579       8.78           96.77\n3   Functional but not in use      1686       3.23          100.00\n\nfreq(data = wp_nonfunctional,\n     input = 'status_clean')\n\n\n\n\n                      status_clean frequency percentage cumulative_perc\n1                   Non-Functional     29385      91.25           91.25\n2 Non-Functional due to dry season      2403       7.46           98.71\n3         Abandoned/Decommissioned       234       0.73           99.44\n4                        Abandoned       175       0.54           99.98\n5 Non functional due to dry season         7       0.02          100.00\n\nfreq(data = wp_unknown,\n     input = 'status_clean')\n\n\n\n\n  status_clean frequency percentage cumulative_perc\n1      unknown     10656        100             100\n\n\n\n\n\n5.2 Performing Point-in-Polygon Count\nNext, we want to find out the number of total, functional, nonfunctional and unknown water points in each LGA. This is performed in the following code chunk. First, it identifies the functional water points in each LGA by using st_intersects() of sf package. Next, length() is used to calculate the number of functional water points that fall inside each LGA.\n\n\nNGA_wp <- NGA %>% \n  mutate(`total_wp` = lengths(\n    st_intersects(NGA, wp_sf_nga))) %>%\n  mutate(`wp_functional` = lengths(\n    st_intersects(NGA, wp_functional))) %>%\n  mutate(`wp_nonfunctional` = lengths(\n    st_intersects(NGA, wp_nonfunctional))) %>%\n  mutate(`wp_unknown` = lengths(\n    st_intersects(NGA, wp_unknown)))\n\n\nNotice that four new derived fields have been added into NGA_wp sf data.frame.\n\n\n5.3 Visualing attributes by using statistical graphs\nIn this code chunk below, appropriate functions of ggplot2 package is used to reveal the distribution of total water points by LGA in histogram.\n\n\nggplot(data = NGA_wp,\n       aes(x = total_wp)) + \n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(\n    total_wp, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             size=0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No. of\\nLGAs\") +\n  theme(axis.title.y=element_text(angle = 0))\n\n\n\n\n\n\n\n5.4 Saving the analytical data in rds format\nIn order to retain the sf object structure for subsequent analysis, it is recommended to save the sf data.frame into rds format.\nIn the code chunk below, write_rds() of readr package is used to export an sf data.frame into rds format.\n\n\nwrite_rds(NGA_wp, \"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#extracting-water-point-data",
    "href": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#extracting-water-point-data",
    "title": "In Class Exercise 2: Geospatial Data Wrangling",
    "section": "5.1 Extracting Water Point Data",
    "text": "5.1 Extracting Water Point Data\nNow we are ready to extract the water point data according to their status.\nThe code chunk below is used to extract functional water point.\n\nwp_functional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\"Functional\",\n             \"Functional but not in use\",\n             \"Functional but needs repair\"))\n\nThe code chunk below is used to extract nonfunctional water point.\n\nwp_nonfunctional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\"Abandoned/Decommissioned\",\n             \"Abandoned\",\n             \"Non-Functional due to dry season\",\n             \"Non-Functional\",\n             \"Non functional due to dry season\"))\n\nThe code chunk below is used to extract water point with unknown status.\n\nwp_unknown <- wp_sf_nga %>%\n  filter(status_clean == \"unknown\")\n\nNext, the code chunk below is used to perform a quick EDA on the derived sf data.frames.\n\nfreq(data = wp_functional,\n     input = 'status_clean')\n\n\n\n\n                 status_clean frequency percentage cumulative_perc\n1                  Functional     45883      87.99           87.99\n2 Functional but needs repair      4579       8.78           96.77\n3   Functional but not in use      1686       3.23          100.00\n\n\n\nfreq(data = wp_nonfunctional,\n     input = 'status_clean')\n\n\n\n\n                      status_clean frequency percentage cumulative_perc\n1                   Non-Functional     29385      91.25           91.25\n2 Non-Functional due to dry season      2403       7.46           98.71\n3         Abandoned/Decommissioned       234       0.73           99.44\n4                        Abandoned       175       0.54           99.98\n5 Non functional due to dry season         7       0.02          100.00\n\n\n\nfreq(data = wp_unknown,\n     input = 'status_clean')\n\n\n\n\n  status_clean frequency percentage cumulative_perc\n1      unknown     10656        100             100"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#performing-point-in-polygon-count",
    "href": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#performing-point-in-polygon-count",
    "title": "In Class Exercise 2: Geospatial Data Wrangling",
    "section": "5.2 Performing Point-in-Polygon Count",
    "text": "5.2 Performing Point-in-Polygon Count\nNext, we want to find out the number of total, functional, nonfunctional and unknown water points in each LGA. This is performed in the following code chunk. First, it identifies the functional water points in each LGA by using st_intersects() of sf package. Next, length() is used to calculate the number of functional water points that fall inside each LGA.\n\nNGA_wp <- NGA %>% \n  mutate(`total_wp` = lengths(\n    st_intersects(NGA, wp_sf_nga))) %>%\n  mutate(`wp_functional` = lengths(\n    st_intersects(NGA, wp_functional))) %>%\n  mutate(`wp_nonfunctional` = lengths(\n    st_intersects(NGA, wp_nonfunctional))) %>%\n  mutate(`wp_unknown` = lengths(\n    st_intersects(NGA, wp_unknown)))\n\nNotice that four new derived fields have been added into NGA_wp sf data.frame."
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#visualising-attributes-by-using-statistical-graphs",
    "href": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#visualising-attributes-by-using-statistical-graphs",
    "title": "In Class Exercise 2: Geospatial Data Wrangling",
    "section": "5.3 Visualising attributes by using statistical graphs",
    "text": "5.3 Visualising attributes by using statistical graphs\nIn this code chunk below, appropriate functions of ggplot2 package is used to reveal the distribution of total water points by LGA in histogram.\n\nggplot(data = NGA_wp,\n       aes(x = total_wp)) + \n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(\n    total_wp, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             size=0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No. of\\nLGAs\") +\n  theme(axis.title.y=element_text(angle = 0))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#saving-the-analytical-data-in-rds-format",
    "href": "In_Class_Ex/In_Class_Ex02/In_Class_Ex02.html#saving-the-analytical-data-in-rds-format",
    "title": "In Class Exercise 2: Geospatial Data Wrangling",
    "section": "5.4 Saving the analytical data in rds format",
    "text": "5.4 Saving the analytical data in rds format\nIn order to retain the sf object structure for subsequent analysis, it is recommended to save the sf data.frame into rds format.\nIn the code chunk below, write_rds() of readr package is used to export an sf data.frame into rds format.\n\nwrite_rds(NGA_wp, \"data/rds/NGA_wp.rds\")"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex03/In_Class_Ex03.html",
    "href": "In_Class_Ex/In_Class_Ex03/In_Class_Ex03.html",
    "title": "In-class Exercise 3: Analytical Mapping",
    "section": "",
    "text": "pacman::p_load(tmap, tidyverse, sf)\n\n\nNGA_wp <- read_rds(\"data/NGA_wp.rds\")\n\n\np1 <- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\n\n\nNGA_wp <- NGA_wp %>%\n  mutate(pct_functional = wp_functional/total_wp) %>%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\ntm_shape(NGA_wp) +\n  tm_fill(\"pct_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)\n\n\n\n\n\nNGA_wp <- NGA_wp %>%\n  drop_na()\n\n\npercent <- c(0,.01,.1,.5,.9,.99,1)\nvar <- NGA_wp[\"pct_functional\"] %>%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.0000000 0.0000000 0.2169811 0.4791667 0.8611111 1.0000000 1.0000000 \n\n\n\nget.var <- function(vname,df) {\n  v <- df[vname] %>% \n    st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\npercentmap <- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent <- c(0,.01,.1,.5,.9,.99,1)\n  var <- get.var(vnam, df)\n  bperc <- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"< 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"> 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\nggplot(data = NGA_wp,\n       aes(x = \"\",\n           y = wp_nonfunctional)) +\n  geom_boxplot()\n\n\n\n\n\nboxbreaks <- function(v,mult=1.5) {\n  qv <- unname(quantile(v))\n  iqr <- qv[4] - qv[2]\n  upfence <- qv[4] + mult * iqr\n  lofence <- qv[2] - mult * iqr\n  # initialize break points vector\n  bb <- vector(mode=\"numeric\",length=7)\n  # logic for lower and upper fences\n  if (lofence < qv[1]) {  # no lower outliers\n    bb[1] <- lofence\n    bb[2] <- floor(qv[1])\n  } else {\n    bb[2] <- lofence\n    bb[1] <- qv[1]\n  }\n  if (upfence > qv[5]) { # no upper outliers\n    bb[7] <- upfence\n    bb[6] <- ceiling(qv[5])\n  } else {\n    bb[6] <- upfence\n    bb[7] <- qv[5]\n  }\n  bb[3:5] <- qv[2:4]\n  return(bb)\n}\n\n\nget.var <- function(vname,df) {\n  v <- df[vname] %>% st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\n\nvar <- get.var(\"wp_nonfunctional\", NGA_wp) \nboxbreaks(var)\n\n[1] -56.5   0.0  14.0  34.0  61.0 131.5 278.0\n\n\n\nboxmap <- function(vnam, df, \n                   legtitle=NA,\n                   mtitle=\"Box Map\",\n                   mult=1.5){\n  var <- get.var(vnam,df)\n  bb <- boxbreaks(var)\n  tm_shape(df) +\n    tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,title=legtitle,\n             breaks=bb,\n             palette=\"Blues\",\n          labels = c(\"lower outlier\", \n                     \"< 25%\", \n                     \"25% - 50%\", \n                     \"50% - 75%\",\n                     \"> 75%\", \n                     \"upper outlier\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"left\",\n                               \"top\"))\n}\n\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nboxmap(\"wp_nonfunctional\", NGA_wp)\n\nWarning: Breaks contains positive and negative values. Better is to use\ndiverging scale instead, or set auto.palette.mapping to FALSE.\n\n\n\n\n\n\nNGA_wp <- NGA_wp %>%\n  mutate(wp_functional = na_if(\n    total_wp, total_wp < 0))"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex04/In_Class_Ex04.html",
    "href": "In_Class_Ex/In_Class_Ex04/In_Class_Ex04.html",
    "title": "In-class Exercise 4",
    "section": "",
    "text": "{pacman::p_load(maptools, sf, raster, spatstat, tmap)}\n\nThings to learn from this code chunk. All explanations have to be outside the code chunk if not will encounter issues"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex04/In_Class_Ex04.html#importing-the-spatial-data",
    "href": "In_Class_Ex/In_Class_Ex04/In_Class_Ex04.html#importing-the-spatial-data",
    "title": "In-class Exercise 4",
    "section": "2. Importing the Spatial Data",
    "text": "2. Importing the Spatial Data\nAlways check the file by right clicking and check the projection system the dataset file is using\n\nchildcare_sf <- st_read(\"data/child-care-services-geojson.geojson\") %>% st_transform(crs = 3414)\n\nReading layer `child-care-services-geojson' from data source \n  `C:\\G2wongkelly\\IS415_GAA\\In_Class_Ex\\In_Class_Ex04\\data\\child-care-services-geojson.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1545 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nsg_sf <- st_read(dsn = \"data\", layer=\"CostalOutline\")\n\nReading layer `CostalOutline' from data source \n  `C:\\G2wongkelly\\IS415_GAA\\In_Class_Ex\\In_Class_Ex04\\data' using driver `ESRI Shapefile'\nSimple feature collection with 60 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 2663.926 ymin: 16357.98 xmax: 56047.79 ymax: 50244.03\nProjected CRS: SVY21\n\n\n\nmpsz_sf <- st_read(dsn = \"data\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\G2wongkelly\\IS415_GAA\\In_Class_Ex\\In_Class_Ex04\\data' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\ntmap_mode('view') #default (not more than 5 in assignment)\ntm_shape(childcare_sf)+ # data layer\n  tm_dots(alpha = 0.5, # different opacity - solid back to grey etc\n          size = 0.01) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n# 11 is the zoom out value (always smaller than the second value)\n# 14 is the zoom in value \n\n\n4.5.1 Converting sf data frames to sp’s Spatial* class\n\nchildcare <- as_Spatial(childcare_sf)\nmpsz <- as_Spatial(mpsz_sf)\nsg <- as_Spatial(sg_sf)\n\n\nchildcare\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1545 \nextent      : 11203.01, 45404.24, 25667.6, 49300.88  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 2\nnames       :    Name,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Description \nmin values  :   kml_1, <center><table><tr><th colspan='2' align='center'><em>Attributes</em></th></tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSBLOCKHOUSENUMBER</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSBUILDINGNAME</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSPOSTALCODE</th> <td>018989</td> </tr><tr bgcolor=\"\"> <th>ADDRESSSTREETNAME</th> <td>1, MARINA BOULEVARD, #B1 - 01, ONE MARINA BOULEVARD, SINGAPORE 018989</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSTYPE</th> <td></td> </tr><tr bgcolor=\"\"> <th>DESCRIPTION</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>HYPERLINK</th> <td></td> </tr><tr bgcolor=\"\"> <th>LANDXADDRESSPOINT</th> <td>0</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>LANDYADDRESSPOINT</th> <td>0</td> </tr><tr bgcolor=\"\"> <th>NAME</th> <td>THE LITTLE SKOOL-HOUSE INTERNATIONAL PTE. LTD.</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>PHOTOURL</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSFLOORNUMBER</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>INC_CRC</th> <td>08F73931F4A691F4</td> </tr><tr bgcolor=\"\"> <th>FMEL_UPD_D</th> <td>20200826094036</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSUNITNUMBER</th> <td></td> </tr></table></center> \nmax values  : kml_999,                  <center><table><tr><th colspan='2' align='center'><em>Attributes</em></th></tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSBLOCKHOUSENUMBER</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSBUILDINGNAME</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSPOSTALCODE</th> <td>829646</td> </tr><tr bgcolor=\"\"> <th>ADDRESSSTREETNAME</th> <td>200, PONGGOL SEVENTEENTH AVENUE, SINGAPORE 829646</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSTYPE</th> <td></td> </tr><tr bgcolor=\"\"> <th>DESCRIPTION</th> <td>Child Care Services</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>HYPERLINK</th> <td></td> </tr><tr bgcolor=\"\"> <th>LANDXADDRESSPOINT</th> <td>0</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>LANDYADDRESSPOINT</th> <td>0</td> </tr><tr bgcolor=\"\"> <th>NAME</th> <td>RAFFLES KIDZ @ PUNGGOL PTE LTD</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>PHOTOURL</th> <td></td> </tr><tr bgcolor=\"\"> <th>ADDRESSFLOORNUMBER</th> <td></td> </tr><tr bgcolor=\"#E3E3F3\"> <th>INC_CRC</th> <td>379D017BF244B0FA</td> </tr><tr bgcolor=\"\"> <th>FMEL_UPD_D</th> <td>20200826094036</td> </tr><tr bgcolor=\"#E3E3F3\"> <th>ADDRESSUNITNUMBER</th> <td></td> </tr></table></center> \n\n\n\n\n4.5.2 Converting the Spatial* class into generic sp format\n\nchildcare_sp <- as(childcare, \"SpatialPoints\")\nsg_sp <- as(sg, \"SpatialPolygons\")\nsg_sp\n\nclass       : SpatialPolygons \nfeatures    : 60 \nextent      : 2663.926, 56047.79, 16357.98, 50244.03  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +datum=WGS84 +units=m +no_defs \n\n\n\n\n4.5.3 Converting the generic sp format into spatstat’s ppp format\nConvert to only x y coordinate in two columns form so that can use in analytics later on\n\nchildcare_ppp <- as(childcare_sp, \"ppp\")\nchildcare_ppp\n\nPlanar point pattern: 1545 points\nwindow: rectangle = [11203.01, 45404.24] x [25667.6, 49300.88] units\n\n\n\nplot(childcare_ppp)\n\n\n\n\n\nsg_owin <- as(sg_sp, \"owin\")\nplot(sg_owin)"
  },
  {
    "objectID": "Take_Home_Ex/Take_Home_Ex01/Take_Home_Ex01.html",
    "href": "Take_Home_Ex/Take_Home_Ex01/Take_Home_Ex01.html",
    "title": "Take-Home Exercise 1: Application of Spatial Point Patterns Analysis to discover geographical distribution of functional & non-functional water points in Osub State, Nigeria",
    "section": "",
    "text": "1. Overview\nWater is an important resource to mankind. Clean and accessible water is critical to human health. It provides a healthy environment, a sustainable economy, reduces poverty and ensures peace and security. Yet over 40% of the global population does not have access to sufficient clean water. By 2025, 1.8 billion people will be living in countries or regions with absolute water scarcity, according to UN-Water. The lack of water poses a major threat to several sectors, including food security. Agriculture uses about 70% of the world’s accessible freshwater.\n\nDeveloping countries are most affected by water shortages and poor water quality. Up to 80% of illnesses in the developing world are linked to inadequate water and sanitation. Despite technological advancement, providing clean water to the rural community is still a major development issues in many countries globally, especially countries in the Africa continent.\nTo address the issue of providing clean and sustainable water supply to the rural community, a global Water Point Data Exchange (WPdx) project has been initiated. The main aim of this initiative is to collect water point related data from rural areas at the water point or small water scheme level and share the data via WPdx Data Repository, a cloud-based data library. What is so special of this project is that data are collected based on WPDx Data Standard.\n\n1.0 Objectives\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate spatial point patterns analysis methods to discover the geographical distribution of functional and non-function water points and their co-locations if any in Osun State, Nigeria.\n\n\n1.1 Data Acquisition\nApstial data\nFor the purpose of this assignment, data from WPdx Global Data Repositories will be used. There are two versions of the data. They are: WPdx-Basic and WPdx+. You are required to use WPdx+ data set.\nGeospatial data\nThis study will focus of Osun State, Nigeria. The state boundary GIS data of Nigeria can be downloaded either from The Humanitarian Data Exchange portal or geoBoundaries.\n\n\n\n2. Getting started\n\n2.1 Installing and Loading the R packages\nFor take-home assignment 1, we will need to install the following packages:\n\npacman::p_load(sf, funModeling,maptools,raster, spatstat, tmap ,  tidyverse, sfdep)\n\nThe code chunk is to check that all the required packages are installed if not, install them.\n\nif (!require(sf)) {\ninstall.packages(\"sf\")\n}\nif (!require(funModeling)) {\ninstall.packages(\"funModeling\")\n}\nif (!require(maptools)) {\ninstall.packages(\"maptools\")\n}\nif (!require(raster)) {\ninstall.packages(\"raster\")\n}\nif (!require(spatstat)) {\ninstall.packages(\"spatstat\")\n}\nif (!require(tmap)) {\ninstall.packages(\"tmap\")\n}\nif (!require(tidyverse)) {\ninstall.packages(\"tidyverse\")\n}\n\n\n\n\n3. Data Wrangling: Geospatial Data & Aspatial Data\n\n\n3.1 Importing geoBoundaries Data into R\nIn this section of 3.1, st_read() of sf package will be used to import geospatial geoboundaries-NGA data set into R.\n\ngeoNGA <- st_read(\"data/geospatial/\",\n                  layer = \"geoBoundaries-NGA-ADM2\")\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `C:\\G2wongkelly\\IS415_GAA\\Take_Home_Ex\\Take_Home_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\nFrom the output message, we learn that:\n\nGeometry type of geoBoundaries dataset is multipolygon\n774 features and 5 fields\nAssigned CRS is WGS 84 (geographic coordinate system)\nDimension is XY\n\n\n\n3.2 Importing Geospatial NGA Data into R\nIn this section of 3.2, st_read() of sf package will be used to import geospatial NGA dataset into R.\nWe filter data to only Osun state as that is what we are interested in finding for this assignment!\n\nNGA <- st_read(\"data/geospatial/\",\n               layer = \"nga_admbnda_adm2_osgof_20190417\") %>%\n  filter(ADM1_EN == \"Osun\") %>% \n  st_transform(crs = 26392)\n\nReading layer `nga_admbnda_adm2_osgof_20190417' from data source \n  `C:\\G2wongkelly\\IS415_GAA\\Take_Home_Ex\\Take_Home_Ex01\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\nFrom the output message, we learn that:\n\nGeometry type of NGA dataset is multipolygon\n774 features and 16 fields\nAssigned CRS is WGS 84 (geographic coordinate system)\nDimension is XY\n\nIn geospatial analytics, we need to transform the original data that is in geographic coordinate system (WGS) to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance and/or area measurements.\nTherefore, we need to transform NGA dataset to projected coordinate system by using st_transform() in sf package. (will be further elaborate in section 3.3.1 and 3.3.2)\nBy examining both sf dataframe closely, we notice that NGA provide both LGA and state information.\nHence, NGA data.frame will be used for the subsequent processing.\n\n\n3.3 Importing Aspatial Data into R\nIn this section of 3.3, read_csv() will be used to import asptial data set into R and we filter out only Nigeria, Osun data rows as those are what we interested in analysing for this project.\n\nwp_nga <- read_csv(\"data/aspatial/WPdx.csv\") %>%\n  filter(`#clean_adm1` == \"Osun\") %>%\n  filter(`#clean_country_name` == \"Nigeria\")\n\n\n3.3.1 Converting Water Point Data into SF Point Features\nStep 1: Convert the wkt field into sfc field by using st_as_sfc() data type.\n\nwp_nga$Geometry = st_as_sfc(wp_nga$`New Georeferenced Column`)\nwp_nga\n\n# A tibble: 5,745 × 75\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n    <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 225950 Federal Minis…    7.43    4.26 05/05/… Yes     Boreho… Well    Hand P…\n 2 225524 Federal Minis…    7.78    4.56 04/22/… Yes     Protec… Well    Hand P…\n 3 197014 Federal Minis…    7.49    4.53 04/30/… Yes     Boreho… Well    Mechan…\n 4 225173 Federal Minis…    7.93    4.73 05/02/… Yes     Boreho… Well    Hand P…\n 5 225843 Federal Minis…    7.74    4.44 05/08/… Yes     Boreho… Well    Hand P…\n 6 235508 Federal Minis…    7.15    4.64 04/27/… Yes     Protec… Well    Hand P…\n 7 197708 Federal Minis…    7.87    4.72 05/13/… Yes     Boreho… Well    Mechan…\n 8 195041 Federal Minis…    7.73    4.45 06/17/… Yes     Protec… Spring  <NA>   \n 9 225222 Federal Minis…    7.81    4.15 05/14/… Yes     Protec… Spring  Mechan…\n10 460770 GRID3             7.4     4.33 06/13/… Unknown Boreho… Well    <NA>   \n# … with 5,735 more rows, 66 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\nStep 2: Convert the tibble data.frame into an sf object by using st_sf(). It is also important for us to include the referencing system of the data into the sf object.\n\nwp_sf <- st_sf(wp_nga, crs=4326)\nwp_sf\n\nSimple feature collection with 5745 features and 74 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 4.032004 ymin: 7.060309 xmax: 5.06 ymax: 8.061898\nGeodetic CRS:  WGS 84\n# A tibble: 5,745 × 75\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n *  <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 225950 Federal Minis…    7.43    4.26 05/05/… Yes     Boreho… Well    Hand P…\n 2 225524 Federal Minis…    7.78    4.56 04/22/… Yes     Protec… Well    Hand P…\n 3 197014 Federal Minis…    7.49    4.53 04/30/… Yes     Boreho… Well    Mechan…\n 4 225173 Federal Minis…    7.93    4.73 05/02/… Yes     Boreho… Well    Hand P…\n 5 225843 Federal Minis…    7.74    4.44 05/08/… Yes     Boreho… Well    Hand P…\n 6 235508 Federal Minis…    7.15    4.64 04/27/… Yes     Protec… Well    Hand P…\n 7 197708 Federal Minis…    7.87    4.72 05/13/… Yes     Boreho… Well    Mechan…\n 8 195041 Federal Minis…    7.73    4.45 06/17/… Yes     Protec… Spring  <NA>   \n 9 225222 Federal Minis…    7.81    4.15 05/14/… Yes     Protec… Spring  Mechan…\n10 460770 GRID3             7.4     4.33 06/13/… Unknown Boreho… Well    <NA>   \n# … with 5,735 more rows, 66 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\n\n\n3.3.2 Transforming into Nigeria Projected Coordinate System\n\nwp_sf <- wp_sf %>%\n  st_transform(crs = 26392)\nwp_sf\n\nSimple feature collection with 5745 features and 74 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 177285.9 ymin: 340054.1 xmax: 291287.1 ymax: 450859.7\nProjected CRS: Minna / Nigeria Mid Belt\n# A tibble: 5,745 × 75\n   row_id `#source`      #lat_…¹ #lon_…² #repo…³ #stat…⁴ #wate…⁵ #wate…⁶ #wate…⁷\n *  <dbl> <chr>            <dbl>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 225950 Federal Minis…    7.43    4.26 05/05/… Yes     Boreho… Well    Hand P…\n 2 225524 Federal Minis…    7.78    4.56 04/22/… Yes     Protec… Well    Hand P…\n 3 197014 Federal Minis…    7.49    4.53 04/30/… Yes     Boreho… Well    Mechan…\n 4 225173 Federal Minis…    7.93    4.73 05/02/… Yes     Boreho… Well    Hand P…\n 5 225843 Federal Minis…    7.74    4.44 05/08/… Yes     Boreho… Well    Hand P…\n 6 235508 Federal Minis…    7.15    4.64 04/27/… Yes     Protec… Well    Hand P…\n 7 197708 Federal Minis…    7.87    4.72 05/13/… Yes     Boreho… Well    Mechan…\n 8 195041 Federal Minis…    7.73    4.45 06/17/… Yes     Protec… Spring  <NA>   \n 9 225222 Federal Minis…    7.81    4.15 05/14/… Yes     Protec… Spring  Mechan…\n10 460770 GRID3             7.4     4.33 06/13/… Unknown Boreho… Well    <NA>   \n# … with 5,735 more rows, 66 more variables: `#water_tech_category` <chr>,\n#   `#facility_type` <chr>, `#clean_country_name` <chr>, `#clean_adm1` <chr>,\n#   `#clean_adm2` <chr>, `#clean_adm3` <chr>, `#clean_adm4` <chr>,\n#   `#install_year` <dbl>, `#installer` <chr>, `#rehab_year` <lgl>,\n#   `#rehabilitator` <lgl>, `#management_clean` <chr>, `#status_clean` <chr>,\n#   `#pay` <chr>, `#fecal_coliform_presence` <chr>,\n#   `#fecal_coliform_value` <dbl>, `#subjective_quality` <chr>, …\n\n\nFrom the output message, we learn that:\n\nGeometry type of NGA dataset is now point\n5745 features and 74 fields\nProjected CRS: Minna/Nigeria Mid Belt\nDimension: XY\n\nWe have successfully transformed the data!! :D\n\n\n\n4. Data Pre-Processing\nBefore we can visualise our dataset and do the necessary analysis, we have to do data cleaning which is an important step in any data science task including geospatial data science. Things to check in the dataset:\n\nInvalid geometries\nExclude redundancy\nMissing value\nDuplicate name\n\n\n4.1 Check for Invalid Geometries\n\nlength(which(st_is_valid(NGA) == FALSE))\n\n[1] 0\n\nlength(which(st_is_valid(wp_sf) == FALSE))\n\n[1] 0\n\n\nFrom the above generated output message, there are no invalid geometries! Great!\n\n\n4.2 Exclude Redundancy\n\nNGA <- NGA %>%\n  select(c(3:4, 8:9))\n\n\n\n4.3 Check for Missing Value\n\nNGA[rowSums(is.na(NGA))!=0,]\n\nSimple feature collection with 0 features and 4 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: Minna / Nigeria Mid Belt\n[1] ADM2_EN    ADM2_PCODE ADM1_EN    ADM1_PCODE geometry  \n<0 rows> (or 0-length row.names)\n\n\nThe printout shows that there is zero missing value in the dataset!\n\n\n4.4 Check for Duplicate Name\n\nNGA$ADM2_EN[duplicated(NGA$ADM2_EN)==TRUE]\n\ncharacter(0)\n\n\nGreat!\nNow, we are ready to analyse the dataset!\n\n\n\n5. Data Wrangling for Water Point Data\nExploratory Data Analysis (EDA) is a popular approach to gain initial understanding of the data.\nFirstly, we take a look at all the column names in wp_sf dataset and identify the column we need to plot status_clean frequency bar chart.\n\ncolnames(wp_sf)\n\n [1] \"row_id\"                      \"#source\"                    \n [3] \"#lat_deg\"                    \"#lon_deg\"                   \n [5] \"#report_date\"                \"#status_id\"                 \n [7] \"#water_source_clean\"         \"#water_source_category\"     \n [9] \"#water_tech_clean\"           \"#water_tech_category\"       \n[11] \"#facility_type\"              \"#clean_country_name\"        \n[13] \"#clean_adm1\"                 \"#clean_adm2\"                \n[15] \"#clean_adm3\"                 \"#clean_adm4\"                \n[17] \"#install_year\"               \"#installer\"                 \n[19] \"#rehab_year\"                 \"#rehabilitator\"             \n[21] \"#management_clean\"           \"#status_clean\"              \n[23] \"#pay\"                        \"#fecal_coliform_presence\"   \n[25] \"#fecal_coliform_value\"       \"#subjective_quality\"        \n[27] \"#activity_id\"                \"#scheme_id\"                 \n[29] \"#wpdx_id\"                    \"#notes\"                     \n[31] \"#orig_lnk\"                   \"#photo_lnk\"                 \n[33] \"#country_id\"                 \"#data_lnk\"                  \n[35] \"#distance_to_primary_road\"   \"#distance_to_secondary_road\"\n[37] \"#distance_to_tertiary_road\"  \"#distance_to_city\"          \n[39] \"#distance_to_town\"           \"water_point_history\"        \n[41] \"rehab_priority\"              \"water_point_population\"     \n[43] \"local_population_1km\"        \"crucialness_score\"          \n[45] \"pressure_score\"              \"usage_capacity\"             \n[47] \"is_urban\"                    \"days_since_report\"          \n[49] \"staleness_score\"             \"latest_record\"              \n[51] \"location_id\"                 \"cluster_size\"               \n[53] \"#clean_country_id\"           \"#country_name\"              \n[55] \"#water_source\"               \"#water_tech\"                \n[57] \"#status\"                     \"#adm2\"                      \n[59] \"#adm3\"                       \"#management\"                \n[61] \"#adm1\"                       \"New Georeferenced Column\"   \n[63] \"lat_deg_original\"            \"lat_lon_deg\"                \n[65] \"lon_deg_original\"            \"public_data_source\"         \n[67] \"converted\"                   \"count\"                      \n[69] \"created_timestamp\"           \"updated_timestamp\"          \n[71] \"#pay_clean\"                  \"#subjective_quality_clean\"  \n[73] \"is_duplicate\"                \"dataset_title\"              \n[75] \"Geometry\"                   \n\n\nNow, once we have the column name “#status_clean”, we use the “table” function to get the frequency of unique values in a vector.\nThis will return a frequency table with unique values in “wp_sf$‘#status_clean’ and their corresponding frequency. The”sort” function will sort the table based on the frequency.\n\nsort(table(wp_sf$\"#status_clean\"), decreasing = TRUE)\n\n\n               Functional            Non-Functional  Functional, needs repair \n                     2406                      2086                       259 \n      Non-Functional, dry    Functional, not in use  Abandoned/Decommissioned \n                      159                        64                        15 \nFunctional but not in use \n                        8 \n\n\nTo plot a bar chart based on the frequency table, use the “barplot” function in R.\nThe below code will create a bar plot of the frequency table, with the x-axis labeled “status_clean” and the y-axis labeled “Frequency”. The main title of the plot will be “Bar Plot of status_clean”.\n\n#Set the colour scheme for the bar\ncolors <- c(\"grey\",\"pink\",\"purple\",\"blue\",\"green\",\"yellow\")\n\n#Plot the frequency of status_clean in bar chart\nfreq_table <- sort(table(wp_sf$\"#status_clean\"), decreasing = TRUE)\nbarplot(freq_table, xlab = \"status_clean\", ylab = \"Frequency\", main = \"Bar Plot of status_clean\", col = colors)\n\n\n\n\nThe below code chunk will include percentage labels on the bar plot!\n\n# calculate the percentage of each status_clean value \nfreq_table_pct <- round(100* prop.table(freq_table),2)\n\n# plot the bar plot with the percentage labels \nbarplot(freq_table, xlab = \"status_clean\", ylab = \"Frequency\", main = \"Bar Plot of status_clean (Percentage)\", col = colors, las = 2)\ntext(x = 1:length(freq_table), y = freq_table + 0.5, labels = paste(freq_table_pct, \"%\"), pos = 3, cex = 0.7)\n\n\n\n\nNext, code chunk below will be used to perform the following data wrangling tasksP - rename() of dplyr package is used to rename the column from #status_clean to status_clean for easier handling in subsequent steps. mutate() and replace_na() are used to recode all the NA values in status_clean into unknown.\n\nwp_sf_nga <- wp_sf %>% \n  rename(status_clean = '#status_clean') %>%\n  select(status_clean) %>%\n  mutate(status_clean = replace_na(\n    status_clean, \"unknown\"))\n\n\n5.1 Extracting Water Point Data\nNow we are ready to extract the water point data according to their status.\nThe code chunk below is used to extract functional water point.\n\nwp_functional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\"Functional\",\n             \"Functional but not in use\",\n             \"Functional but needs repair\"))\n\nThe code chunk below is used to extract nonfunctional water point.\n\nwp_nonfunctional <- wp_sf_nga %>%\n  filter(status_clean %in%\n           c(\"Abandoned/Decommissioned\",\n             \"Abandoned\",\n             \"Non-Functional due to dry season\",\n             \"Non-Functional\",\n             \"Non functional due to dry season\"))\n\nThe code chunk below is used to extract water point with unknown status.\n\nwp_unknown <- wp_sf_nga %>%\n  filter(status_clean == \"unknown\")\n\n\n\n5.2 Performing Point-in-Polygon Count\nNext, we want to find out the number of total, functional, nonfunctional and unknown water points in each LGA. This is performed in the following code chunk.\nFirst, it identifies the functional water points in each LGA by using st_intersects() of sf package. Next, length() is used to calculate the number of functional water points that fall inside each LGA.\n\nNGA_wp <- NGA %>% \n  mutate(`total_wp` = lengths(\n    st_intersects(NGA, wp_sf_nga))) %>%\n  mutate(`wp_functional` = lengths(\n    st_intersects(NGA, wp_functional))) %>%\n  mutate(`wp_nonfunctional` = lengths(\n    st_intersects(NGA, wp_nonfunctional))) %>%\n  mutate(`wp_unknown` = lengths(\n    st_intersects(NGA, wp_unknown)))\n\nNotice that four new derived fields have been added into NGA_wp sf data.frame.\nWe can visualise the summary of NGA_wp sf dataframe in statistics forms such as mean, median, and max etc for both functional and nonfunctional by using summary() as shown in the code chunk below:\n\nsummary(NGA_wp)\n\n   ADM2_EN           ADM2_PCODE          ADM1_EN           ADM1_PCODE       \n Length:30          Length:30          Length:30          Length:30         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n          geometry     total_wp     wp_functional    wp_nonfunctional\n MULTIPOLYGON :30   Min.   : 60.0   Min.   : 14.00   Min.   : 17.0   \n epsg:26392   : 0   1st Qu.:134.0   1st Qu.: 50.50   1st Qu.: 46.5   \n +proj=tmer...: 0   Median :166.5   Median : 75.00   Median : 58.0   \n                    Mean   :182.9   Mean   : 77.20   Mean   : 65.9   \n                    3rd Qu.:212.0   3rd Qu.: 91.75   3rd Qu.: 86.5   \n                    Max.   :443.0   Max.   :249.00   Max.   :140.0   \n   wp_unknown   \n Min.   : 4.00  \n 1st Qu.:13.00  \n Median :22.00  \n Mean   :24.47  \n 3rd Qu.:32.75  \n Max.   :78.00  \n\n\n\n\n5.3 Visualising attributes by using statistical graphs\n\nggplot(data = NGA_wp,\n       aes(x = total_wp)) + \n  geom_histogram(bins=20,\n                 color=\"black\",\n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept=mean(\n    total_wp, na.rm=T)),\n             color=\"red\", \n             linetype=\"dashed\", \n             size=0.8) +\n  ggtitle(\"Distribution of total water points by LGA\") +\n  xlab(\"No. of water points\") +\n  ylab(\"No. of\\nLGAs\") +\n  theme(axis.title.y=element_text(angle = 0))\n\n\n\n\n\n\n5.3.1 Observation from Statistical graph of NGA waterpoints\n\nThe histogram is a right-skewed distribution where the long tail extends to the right whole most values cluster on the left, as shown above in section 5.3.\nThere are a few possible outliers in the above histogram and we can use the 1.5 interquartile range (IQR) criterion to check whether they can be considered as outliers.\n\nQ1 = 134 and Q3 = 212, which give an IQR = Q3-Q1 = 78\nQ1 - 1.5(IQR) = 134 - (1.5)(78) = 17\nQ3 + 1.5(IQR) = 212 + (1.5)(78) = 329\n\nThe 1.5(IQR) criterion tells us that any observation with water points that is below 17 or above 329 is considered a suspected outlier.\n\nWith that, we can conclude that there are three outliers in this histogram and those with an arrow above are the ones.\n\n\n\n\n5.4 Saving the analytical data in rds format\n\nwrite_rds(NGA_wp, \"data/rds/NGA_wp.rds\")\n\n\n\n\n6. Geospatial Mapping\n\n6.1 Basic Choropleth Mapping\nIn this section, will be plotting different choropleth maps to analyse the distribution of water point in Nigeria, Osun state.\n\ntmap_mode(\"plot\")\nqtm(NGA_wp, \n    fill = c(\"wp_functional\",\"wp_nonfunctional\", \"wp_unknown\"))\n\n\n\n\n\np1 <- tm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of functional water point by LGAs\",\n            legend.outside = FALSE)\n\n\np2 <- tm_shape(NGA_wp) +\n  tm_fill(\"total_wp\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Distribution of total  water point by LGAs\",\n            legend.outside = FALSE)\n\n\ntmap_arrange(p2, p1, nrow = 1)\n\n\n\n\n\n6.1.1 Observation of the Distribution of Water Points in Nigeria, Osun state.\n\nFrom the map generated earlier in section 6.1 that shows the distribution of water points in Nigeria, Osun state according to their functionality. We observed a few things:\n\nAt first glance, non-functional water map has a higher intensity based on its colours spread compared to the other two maps on its right and left. That could means that non-functional water points are more widely spread in Nigeria, Osun state.\nOn the other hand, generally, functional water map has a least colours intensity spread compared to the other two maps. That could means that functional water points are less common/least spread in Nigeria, Osun state.\n\nIn conclusion, Nigeria, Osun state has a higher level of non-functional water than functional water.\n\n\n\n6.2 Choropleth Map for Rates: Deriving Proportion of Functional Water Points and Non-Functional Water Points\nWe will tabulate the proportion of functional water points and the proportion of non-functional water points in each LGA. In the following code chunk, mutate() from dplyr package is used to derive two fields, namely pct_functional and pct_nonfunctional.\n\nNGA_wp <- NGA_wp %>%\n  mutate(pct_functional = wp_functional/total_wp) %>%\n  mutate(pct_nonfunctional = wp_nonfunctional/total_wp)\n\n\ntm_shape(NGA_wp) +\n  tm_fill(\"wp_functional\",\n          n = 10,\n          style = \"equal\",\n          palette = \"Blues\",\n          title = \"Dependency ratio\",\n          legend.hist = TRUE) +\n  tm_borders(lwd = 0.1,\n             alpha = 1) +\n  tm_layout(main.title = \"Rate map of functional water point by LGAs\",\n            legend.outside = TRUE)\n\n\n\n\n\n\n6.3 Extreme Value Maps: Percentile Map\nExtreme value maps are variations of common choropleth maps where the classification is designed to highlight extreme values at the lower and upper end of the scale, with the goal of identifying outliers. These maps were developed in the spirit of spatializing EDA, i.e., adding spatial features to commonly used approaches in non-spatial EDA (Anselin 1994).\nThe percentile map is a special type of quantile map with six specific categories: 0-1%,1-10%, 10-50%,50-90%,90-99%, and 99-100%. The corresponding breakpoints can be derived by means of the base R quantile command, passing an explicit vector of cumulative probabilities as c(0,.01,.1,.5,.9,.99,1). Note that the begin and endpoint need to be included.\nStep 1: Exclude records with NA by using the code chunk below\n\nNGA_wp <- NGA_wp %>%\n  drop_na()\n\nStep 2: Creating customised classification and extracting values\n\npercent <- c(0,.01,.1,.5,.9,.99,1)\nvar <- NGA_wp[\"pct_functional\"] %>%\n  st_set_geometry(NULL)\nquantile(var[,1], percent)\n\n       0%        1%       10%       50%       90%       99%      100% \n0.2179487 0.2224103 0.2900820 0.4145480 0.5076962 0.6203446 0.6441441 \n\n\nStep 3: Creating the get.var function\nWe will write an R function as shown below to extract a variable (i.e. wp_nonfunctional) as a vector out of an sf data.frame.\n\narguments:\n\nvname: variable name (as character, in quotes)\ndf: name of sf data frame\n\nreturns:\n\nv: vector with values (without a column name)\n\n\n\nget.var <- function(vname,df) {\n  v <- df[vname] %>% \n    st_set_geometry(NULL)\n  v <- unname(v[,1])\n  return(v)\n}\n\nStep 4: we will write a percentile mapping function by using the code chunk below.\n\npercentmap <- function(vnam, df, legtitle=NA, mtitle=\"Percentile Map\"){\n  percent <- c(0,.01,.1,.5,.9,.99,1)\n  var <- get.var(vnam, df)\n  bperc <- quantile(var, percent)\n  tm_shape(df) +\n  tm_polygons() +\n  tm_shape(df) +\n     tm_fill(vnam,\n             title=legtitle,\n             breaks=bperc,\n             palette=\"Blues\",\n          labels=c(\"< 1%\", \"1% - 10%\", \"10% - 50%\", \"50% - 90%\", \"90% - 99%\", \"> 99%\"))  +\n  tm_borders() +\n  tm_layout(main.title = mtitle, \n            title.position = c(\"right\",\"bottom\"))\n}\n\nStep 5: Test drive the percentile mapping function\n\npercentmap(\"total_wp\", NGA_wp)\n\n\n\n\n\n\n\n7. First-Order Spatial Point Patterns Analysis Methods\nVisualising the sf layers\nIt is always a good practice to plot the output sf layers on OSM layer to ensure that they have been imported properly and been projected on an appropriate projection system.\n\ntmap_mode(\"view\")\ntm_shape(wp_functional) +\n  tm_dots(alph = 0.5, \n          size=0.01,\n          border.col = \"blue\",\n          border.lwd = 0.5) +\n  tm_shape(wp_nonfunctional) +\n  tm_dots(alph = 0.5, \n          size=0.01,\n          border.col = \"yellow\",\n          border.lwd = 0.5) +\n  tm_view(set.zoom.limits = c(8,12))\n\n\n\n\n\n\n\n7.1 Converting SF Data Frames to SP’s Spatial Class\nThe code chunk below uses as_Spatial() of sf package to convert the geospatial data from simple data feature data frame to sp’s Spatial* class.\n\nnga_sp <- as_Spatial(wp_sf_nga)\nnga_sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 5745 \nextent      : 177285.9, 291287.1, 340054.1, 450859.7  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \nvariables   : 1\nnames       :             status_clean \nmin values  : Abandoned/Decommissioned \nmax values  :                  unknown \n\n\n\nnga_functional_sp <- as_Spatial(wp_functional)\nnga_functional_sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 2414 \nextent      : 177285.9, 290751, 343128.1, 450859.7  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \nvariables   : 1\nnames       :              status_clean \nmin values  :                Functional \nmax values  : Functional but not in use \n\n\n\nnga_nonfunctional_sp <- as_Spatial(wp_nonfunctional)\nnga_nonfunctional_sp\n\nclass       : SpatialPointsDataFrame \nfeatures    : 2101 \nextent      : 180539, 290546.5, 340054.1, 450780.1  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \nvariables   : 1\nnames       :             status_clean \nmin values  : Abandoned/Decommissioned \nmax values  :           Non-Functional \n\n\nNotice from the output message that the geospatial data wp_sf_nga, wp_functional, and wp_nonfunctional have all been converted to sp’s spatial* class now.\n\n\n7.2 Converting the Spatial* Class into Generic SP Format\nSpstat requires the analytical data in ppp object form. There is no direct way to convert a Spatial* classes into ppp object. We need to convert the Spatial classes* into Spatial object first.\nThe code chunk below converts the Spatial* class of NGA into generic sp object.\n\nnga_sp <- as(nga_sp, \"SpatialPoints\")\nnga_sp\n\nclass       : SpatialPoints \nfeatures    : 5745 \nextent      : 177285.9, 291287.1, 340054.1, 450859.7  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \n\n\n\nnga_functional_sp <- as(nga_functional_sp, \"SpatialPoints\")\nnga_functional_sp\n\nclass       : SpatialPoints \nfeatures    : 2414 \nextent      : 177285.9, 290751, 343128.1, 450859.7  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \n\n\n\nnga_nonfunctional_sp <- as(nga_nonfunctional_sp, \"SpatialPoints\")\nnga_nonfunctional_sp\n\nclass       : SpatialPoints \nfeatures    : 2101 \nextent      : 180539, 290546.5, 340054.1, 450780.1  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=4 +lon_0=8.5 +k=0.99975 +x_0=670553.98 +y_0=0 +a=6378249.145 +rf=293.465 +towgs84=-92,-93,122,0,0,0,0 +units=m +no_defs \n\n\n\n\n7.3 Converting the Generic SP Format into Spatstat’s ppp Format\nNow, we will use as.ppp() function of spatstat to convert the spatial data into spatstat’s ppp object format.\n\nnga_ppp <- as(nga_sp, \"ppp\")\nnga_ppp\n\nPlanar point pattern: 5745 points\nwindow: rectangle = [177285.9, 291287.05] x [340054.1, 450859.7] units\n\n\n\nnga_functional_ppp <- as(nga_functional_sp, \"ppp\")\nnga_functional_ppp\n\nPlanar point pattern: 2414 points\nwindow: rectangle = [177285.9, 290750.96] x [343128.1, 450859.7] units\n\n\n\nnga_nonfunctional_ppp <- as(nga_nonfunctional_sp, \"ppp\")\nnga_nonfunctional_ppp\n\nPlanar point pattern: 2101 points\nwindow: rectangle = [180538.96, 290546.54] x [340054.1, 450780.1] units\n\n\nLet us then plot nga_ppp and examine the different.\n\nplot(nga_ppp)\n\n\n\nplot(nga_functional_ppp)\n\n\n\nplot(nga_nonfunctional_ppp)\n\n\n\n\nWe can also look at the summary statistics of the newly created ppp object by using the code chunk below.\n\nsummary(nga_ppp)\n\nPlanar point pattern:  5745 points\nAverage intensity 4.547987e-07 points per square unit\n\nCoordinates are given to 2 decimal places\ni.e. rounded to the nearest multiple of 0.01 units\n\nWindow: rectangle = [177285.9, 291287.05] x [340054.1, 450859.7] units\n                    (114000 x 110800 units)\nWindow area = 1.2632e+10 square units\n\n\nThere are no warning message about duplicates but let’s do a double check before moving on :)\n\nany(duplicated(nga_ppp))\n\n[1] FALSE\n\n\nFrom the generated output, we can confidently say that there is no duplication!\n7.4 Creating owin object\nWhen analysing spatial point patterns, it is a good practice to confine the analysis with a geographical area like Nigeria boundary. In spatstat, an object called owin is specially designed to represent this polygonal region.\nThe code chunk below is used to convert nga SpatialPolygon object into owin object of spatstat.\nONLY NIGERIA - OSUN STATE**\n\nlibrary(sf)\nlibrary(spatstat)\n\n# Load the spatial point data\nnga_spp <- st_as_sf(NGA, coords = c(\"x\", \"y\"))\n\n# Convert the \"SpatialPoints\" object to a polygon format using st_cast()\nnga_poly <- st_cast(nga_spp, \"POLYGON\")\n\n# Extract the polygon geometry component of the \"sf\" object\nnga_geom <- st_geometry(nga_poly)\n\n# Convert the polygon geometry component to a list of polygon objects\nnga_list_poly <- as.list(nga_geom)\n\n# Create an \"owin\" object from the list of polyggon objects\nnga_owin <- as.owin(nga_spp)\n\n# Analyze the point pattern using spatstat functions\nplot(nga_owin)\n\n\n\n\nThe code chunk below is used to convert nga_geometry spatialpolygon object into owin object of spatstat.\nONLY NIGERIA**\n\nlibrary(sf)\nlibrary(spatstat)\n\n# Load the spatial point data\ngeometry_spp <- st_as_sf(geoNGA, coords = c(\"x\", \"y\"))\n\n# Convert the \"SpatialPoints\" object to a polygon format using st_cast()\nnga_poly <- st_cast(geometry_spp, \"POLYGON\")\n\n# Extract the polygon geometry component of the \"sf\" object\ngeom <- st_geometry(nga_poly)\n\n# Convert the polygon geometry component to a list of polygon objects\nnga_list_poly <- as.list(geom)\n\n#converting the spatial point into a projected coordinate system using the st_transform from the 'sf' package.\nnga_poly_proj <- st_transform(nga_poly, crs = \"+proj=utm +zone=30 +ellps=WGS84\")\n\n# Create an \"owin\" object from the list of polyggon objects\nnga_geometry_owin <- as.owin(nga_poly_proj)\n\n# Analyze the point pattern using spatstat functions\nplot(nga_geometry_owin)\n\n\n\n\n\nnga_ppp = nga_ppp[nga_owin]\nplot(nga_ppp)\n\n\n\n\n\n\n7.5 Kernel Density Estimation\nIn this section, we will learn how to compute the kernel density estimation (KDE). Some definitions:\n\nDensity: The amount of features or events per unit area\nDensity estimation: The construction of the density function from the observed data\nKernel: A window function fitted on each observation (weighted or unweighted) to determine the fraction of the observation used for density estimation at any location within the window\n\nThe code chunk below computes a kernel density by using the following configurations of density() of spatstat:\n\nbw.diggle() automatic bandwidth selection method. Other recommended methods are bw.CvL(), bw.scott() or bw.ppl().\nThe smoothing kernel used is gaussian, which is the default. Other smoothing methods are: “epanechnikov”, “quartic” or “disc”.\nThe intensity estimate is corrected for edge effect bias by using method described by Jones (1993) and Diggle (2010, equation 18.9). The default is FALSE.\n\n\nkde_nga_bw <- density(nga_ppp,\n                      sigma=bw.diggle,\n                      edge=TRUE,\n                    kernel=\"gaussian\")\n\nkde_nga_functional_bw <- density(nga_functional_ppp,\n                      sigma=bw.diggle,\n                      edge=TRUE,\n                    kernel=\"gaussian\")\n\nkde_nga_nonfunctional_bw <- density(nga_nonfunctional_ppp,\n                      sigma=bw.diggle,\n                      edge=TRUE,\n                    kernel=\"gaussian\")\n\nThe plot() function of Base R is then used to display the kernel density derived.\n\nplot(kde_nga_bw)\n\n\n\nplot(kde_nga_functional_bw)\n\n\n\nplot(kde_nga_nonfunctional_bw)\n\n\n\n\n\nbw <- bw.diggle(nga_ppp)\nbw\n\n   sigma \n175.2226 \n\n\n\nbw_functional <-bw.diggle(nga_functional_ppp)\nbw_functional\n\n   sigma \n263.5313 \n\n\n\nbw_nonfunctional <-bw.diggle(nga_nonfunctional_ppp)\nbw_nonfunctional\n\n   sigma \n296.0087 \n\n\n\nnga_ppp.km <- rescale(nga_ppp, 100, \"km\")\n\n\nkde_nga.bw <- density(nga_ppp.km, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nkde_nga_functional.bw <- density(nga_functional_ppp, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\nkde_nga_nonfunctional.bw <- density(nga_nonfunctional_ppp, sigma=bw.diggle, edge=TRUE, kernel=\"gaussian\")\n\nplot(kde_nga.bw)\n\n\n\nplot(kde_nga_functional.bw)\n\n\n\nplot(kde_nga_nonfunctional.bw)\n\n\n\n\n\ngridded_kde_nga_bw <- as.SpatialGridDataFrame.im(kde_nga.bw)\nspplot(gridded_kde_nga_bw)\n\n\n\n\n\nkde_nga.bw_rastor<- raster(gridded_kde_nga_bw)\nkde_nga.bw_rastor\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 8.948485, 9.616045  (x, y)\nextent     : 1765.032, 2910.438, 3314.347, 4545.201  (xmin, xmax, ymin, ymax)\ncrs        : NA \nsource     : memory\nnames      : v \nvalues     : -6.500989e-17, 0.4183645  (min, max)\n\n\n\nprojection(kde_nga.bw_rastor) <- CRS(\"+init=EPSG:3414\")\nkde_nga.bw_rastor\n\nclass      : RasterLayer \ndimensions : 128, 128, 16384  (nrow, ncol, ncell)\nresolution : 8.948485, 9.616045  (x, y)\nextent     : 1765.032, 2910.438, 3314.347, 4545.201  (xmin, xmax, ymin, ymax)\ncrs        : +init=EPSG:3414 \nsource     : memory\nnames      : v \nvalues     : -6.500989e-17, 0.4183645  (min, max)\n\n\n\ntm_shape(kde_nga.bw_rastor) + \n  tm_raster(\"v\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\"), frame = FALSE)\n\n\n\n\n\n\n7.5.1 Observations from Kernel Density Map\n\nDescribe the spatial patterns revealed by the kernel density map\n\nFrom the density map generated, we could tell that the distribution of water points are mostly distributed evenly across Nigeria, Osun state (between 0.10 to 0.15) with only a few spots that have higher intense colours (between 0.25 to 0.30).\nThere are still many areas in Nigeria, Osun state that have zero distribution of water points which means they do not have access to any water (0).\n\nHighlight the advantages of kernel density map over point map\n\nProvides a continuous surface that represents the estimated density of points at each location. This allows to see the overall distribution of water points acorss Nigeria, Osun state more precisely with high or low densities. In contrast, a point map simply displays individual points, which can make it difficult to see patterns in the data.\nHelp identify hot spots or areas of high concentration, which can be useful for identifying clusters of points or areas of interest. This can be particularly useful in this assignment to identify areas with high intensity of water points in Nigeria, Osun state.\nMore visually appealing and easier to interpret than a point map, especially when dealing with large numbers of points.\n\n\nIn conclusion, a kernel density map can provide a more accurate representation of the spatial distribution of points and can be more useful for identifying patterns and hot spots in the data. However, a point map can still be useful for exploring the individual points and for understanding the distribution at the local scale. The choice between a kernel density map and a point map often depends on the specific needs of the analysis and the type of data being analyzed.\n\n\n\n\n8. Second Spatial Point Patterns Analysis Methods\nTo confirm the observed spatial patterns above, a hypothesis test will be conducted. The hypothesis are test as follows:\nHo = The distribution of water points in Nigeria, Osun state are randomly distributed\nH1 = The distribution of water points in Nigeria, Osun sate are not randomly distributed\nConfidence level = 95%\n\n8.1 Analysing Spatial Point Process Using G-Function\nThe code chunk below is used to compute G-function using Gest() of spatat package.\n\nG_nga_functional = Gest(nga_functional_ppp, correction = \"border\")\nplot(G_nga_functional, xlim=c(0,1900))\n\n\n\n\n\nG_nga_nonfunctional = Gest(nga_nonfunctional_ppp, correction = \"border\")\nplot(G_nga_nonfunctional, xlim=c(0,1900))\n\n\n\n\n\n8.1.1 Performing Complete Spatial Randomness Test\nThe null hypothesis will be rejected if p-value is smaller than alpha value of 0.001.\nMonte Carlo test with G-function\n\nG_nga_functional.csr <- envelope(nga_functional_ppp, Gest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60........\n.70.........80.........90.........100.........110.........120.........130......\n...140.........150.........160.........170.........180.........190.........200....\n.....210.........220.........230.........240.........250.........260.........270..\n.......280.........290.........300.........310.........320.........330.........340\n.........350.........360.........370.........380.........390.........400........\n.410.........420.........430.........440.........450.........460.........470......\n...480.........490.........500.........510.........520.........530.........540....\n.....550.........560.........570.........580.........590.........600.........610..\n.......620.........630.........640.........650.........660.........670.........680\n.........690.........700.........710.........720.........730.........740........\n.750.........760.........770.........780.........790.........800.........810......\n...820.........830.........840.........850.........860.........870.........880....\n.....890.........900.........910.........920.........930.........940.........950..\n.......960.........970.........980.........990........ 999.\n\nDone.\n\n\n\nG_nga_nonfunctional.csr <- envelope(nga_nonfunctional_ppp, Gest, nsim = 999)\n\nGenerating 999 simulations of CSR  ...\n1, 2, 3, ......10.........20.........30.........40.........50.........60........\n.70.........80.........90.........100.........110.........120.........130......\n...140.........150.........160.........170.........180.........190.........200....\n.....210.........220.........230.........240.........250.........260.........270..\n.......280.........290.........300.........310.........320.........330.........340\n.........350.........360.........370.........380.........390.........400........\n.410.........420.........430.........440.........450.........460.........470......\n...480.........490.........500.........510.........520.........530.........540....\n.....550.........560.........570.........580.........590.........600.........610..\n.......620.........630.........640.........650.........660.........670.........680\n.........690.........700.........710.........720.........730.........740........\n.750.........760.........770.........780.........790.........800.........810......\n...820.........830.........840.........850.........860.........870.........880....\n.....890.........900.........910.........920.........930.........940.........950..\n.......960.........970.........980.........990........ 999.\n\nDone.\n\n\n\nplot(G_nga_functional.csr,xlim=c(0,1900))\n\n\n\nplot(G_nga_nonfunctional.csr,xlim=c(0,1900))\n\n\n\n\n\n\n8.1.2 Analysis of Spatial Point Pattern using G-Function\nThe G-function creates a graph of the “edge-corrected G-function”. The upper curve is the empirical distribution of nearest neighbor distances for the water points, adjusted for edge effects caused by a finite domain.\nThe lower curve shows the expected distribution for random uniform data of the same size on the same domain. The light-blue band is a 95% confidence envelope, which gives you a feeling for the variation due to random sampling.\nThe G line is above the envelope and spatial randomness line for both functional and non-functional G graphs therefore, the graphs indicate that there is no structure in the water point data and there is a complete spatial randomness.\nIn summary, I will accept my null hypothesis and reject the alternative hypothesis to conclude that the distribution of water points in Nigeria, Osun state are randomly distributed.\nG-function is useful for analyzing properties of spatial point patterns. This article compared tree data to a single instance of random uniform data. By using the SPP procedure, you can run a more complete analysis and obtain graphs and related statistics with minimal effort.\n\n\n\n8.2 Analysing Spatial Point Process Using L-Function\nAccording to lesson 4 slides,\n\nL(r) >0 indicates that the observed distribution is geographically concentrated\nL(r) <0 implies dispersion\nL(r) = 0 indicates complete spatial randomness(CRS)\n\n\nL_nga_functional = Lest(nga_functional_ppp, correction= \"Ripley\")\nplot(L_nga_functional, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\nL_nga_nonfunctional = Lest(nga_nonfunctional_ppp, correction= \"Ripley\")\nplot(L_nga_nonfunctional, . -r ~ r, \n     ylab= \"L(d)-r\", xlab = \"d(m)\")\n\n\n\n\n\nL_nga_nonfunctional.csr <- envelope(nga_nonfunctional_ppp, Lest, nsim = 39, rank = 1, glocal=TRUE)\n\nGenerating 39 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,  39.\n\nDone.\n\n\n\nL_nga_functional.csr <- envelope(nga_functional_ppp, Lest, nsim = 39, rank = 1, glocal=TRUE)\n\nGenerating 39 simulations of CSR  ...\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,  39.\n\nDone.\n\n\n\nplot(L_nga_functional.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n\nplot(L_nga_nonfunctional.csr, . - r ~ r, xlab=\"d\", ylab=\"L(d)-r\")\n\n\n\n\n8.2.1 Analysis of Spatial Point using L-Function\nFor distance more than 1000m, the L(d) - r function (black line) lies above the L(d) - r function at CRS (red line).\nTherefore, I will accept my null hypothesis and reject the alternative hypothesis to conclude that the distribution of water points in Nigeria, Osun state are randomly distributed.\nFor both functions (G&L), I will accept null hypothese and reject the alternative hypothesis.\n\n\n\n9. Spatial Correlation Analysis\nWhat is Local Colocation Quotients (LCLQ):\nA point event category A is colocated with point events of category B if it is surrounded by several point event category B within a specified distance.\nE.g.\n\nIn this assignment, we are interested to find out if the spatial distribution of functional and non-functional water points are independent from each other.\nTo confirm the observed spatial correlation pattern, a hypothesis test will be conducted. The hypothesis are test as follows:\nHo = The distribution of functional and non-functional water points are independent from each other\nH1 = The distribution of functional and non-functional water points are not independent from each other\nConfidence level = 95%\n\nwp_sf_clean <- wp_sf_nga %>%  filter(!status_clean=='unknown')\n\nnb = include_self(st_knn(st_geometry(wp_sf_clean), 6)) \n\nwt = st_kernel_weights(nb, wp_sf_clean, \"gaussian\", adaptive = TRUE)\nf = wp_sf_clean %>%\n  filter(status_clean == \"Functional\")\n\nA = f$status_clean\n\nnf = wp_sf_clean %>%\n  filter(status_clean == \"Non-Functional\")\nB = nf$status_clean\n\nLCLQ = local_colocation(A, B, nb, wt, 49)\nLCLQ_wp = cbind(wp_sf_clean, LCLQ)\n\nCode breakdown:\n\nThe code first filters the wp_sf_clean object to remove any entries with a “status_clean” of “unknown”.\nThen, it calculates the 6 nearest neighbors for each feature in the wp_sf_clean object using the st_knn function from the sf library and creates a weight matrix for each feature with the st_kernel_weights function, using a Gaussian kernel.\nNext, the code creates two separate objects, f and nf, which contain only the “Functional” and “Non-Functional” features from the wp_sf_clean object, respectively.\nThe local_colocation function is then applied to the two objects, using the nearest neighbor information and weight matrix calculated earlier, with a neighborhood size of 49. Finally, the output of the local_colocation function is combined with the wp_sf_clean object using cbind, creating a new object LCLQ_wp.\n\n\ntmap_mode(\"view\")\ntm_shape(NGA_wp) +\n  tm_polygons() +\n  tm_shape(LCLQ_wp) +\n  tm_dots(col = c(\"Non.Functional\"), \n          size = 0.01,\n          border.col = \"black\",\n          border.lwd = 0.5) +\n  tm_view(set.bounds = c(4,7,5,8),\n          set.zoom.limits = c(8, 13))\n\n\n\n\n\n\n\n9.1 Analysis of Spatial Correlation\nThe tool will determine, for each feature of the Category of Interest, whether the features of the Neighboring Category are more or less present in its neighborhood compared to the overall spatial distribution of the categories. For example, for each feature of category A, a resulting local colocation quotient (LCLQ) value of 1 means that you are as likely to have category B as a neighbor as you might expect. A LCLQ value greater than 1 means you are more likely (than random) to have B as a neighbor, and a LCLQ value less than 1 means that the feature of category A is less likely to have a category B point as your neighbor (than a random distribution).\nAccording to the map generated, the proportion of A (functional points) within the neighborhood of B (nonfunctional points) is higher than the global porportion of A, the colocation quotient is therefore high or based on the slides explanation, 99% of the point have a value of 1 which means that it is likely to have both functional and nonfunctional points collocate together.\nTherefore, I will reject the null hypothesis and accept the alternative hypothesis saying that the distribution of functional and non-functional water points are not independent from each other.\n\n\n\n10. References & Resources Used\nHere are the list of resources used in this analysis, as well as their links. Special thanks to Seniors work samples and Prof Kam for all the detailed explanations and clear documentary posted!! :))\nSection 5.3.1: https://bolt.mph.ufl.edu/6050-6052/unit-1/one-quantitative-variable-introduction/understanding-outliers/\nSection 7.5: https://gistbok.ucgis.org/bok-topics/kernels-and-density-estimation\nSection 8.1: https://blogs.sas.com/content/iml/2016/09/19/nearest-neighbor-distances.html\nSection 8.2.1: https://www.mattpeeples.net/modules/PointPattern.html\nSection 9.1: https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/colocationanalysis.htm"
  },
  {
    "objectID": "Hands_on_Ex/Hands_on_Ex01.html",
    "href": "Hands_on_Ex/Hands_on_Ex01.html",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to handle geospatial data in R by using sf package.\nBy the end of this hands-on exercise, you should acquire the following competencies:\n\ninstalling and loading sf and tidyverse packages into R environment,\nimporting geospatial data by using appropriate functions of sf package,\nimporting aspatial data by using appropriate function of readr package,\nexploring the content of simple feature data frame by using appropriate Base R and sf functions,\nassigning or transforming coordinate systems by using using appropriate sf functions,\nconverting an aspatial data into a sf data frame by using appropriate function of sf package,\nperforming geoprocessing tasks by using appropriate functions of sf package,\nperforming data wrangling tasks by using appropriate functions of dplyr package and\nperforming Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package."
  },
  {
    "objectID": "Hands_on_Ex/Hands_on_Ex01.html#data-acquisition",
    "href": "Hands_on_Ex/Hands_on_Ex01.html#data-acquisition",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling",
    "section": "1.2 Data Acquisition",
    "text": "1.2 Data Acquisition\nData are key to data analytics including geospatial analytics. Hence, before analysing, we need to assemble the necessary data. In this hands-on exercise, you are required to extract the necessary data sets from the following sources:\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\n\n1.2.1 Extracting the geospatial data sets\nNext, at the Hands-on_Ex01 folder, create a sub-folder called data. Then, inside the data sub-folder, create two sub-folders and name them geospatial and aspatial respectively.\nPlace Master Plan 2014 Subzone Boundary (Web), Pre-Schools Location and Cycling Path zipped files into geospatial sub-folder and unzipped them. Copy the unzipped files from their respective sub-folders and place them inside geospatial sub-folder.\n\n\n1.2.2 Extracting the aspatial data set\nNow, you will extract the downloaded listing data file. At Downloads folder, cut and paste listing.csv into aspatial sub-folder."
  },
  {
    "objectID": "Hands_on_Ex/Hands_on_Ex01.html#getting-started",
    "href": "Hands_on_Ex/Hands_on_Ex01.html#getting-started",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling",
    "section": "1.3 Getting Started",
    "text": "1.3 Getting Started\nIn this hands-on exercise, two R packages will be used. They are:\n\nsf for importing, managing, and processing geospatial data, and\ntidyverse for performing data science tasks such as importing, wrangling and visualising data.\n\nTidyverse consists of a family of R packages. In this hands-on exercise, the following packages will be used:\n\nreadr for importing csv data,\nreadxl for importing Excel worksheet,\ntidyr for manipulating data,\ndplyr for transforming data, and\nggplot2 for visualising data\n\n\npacman::p_load(sf, tidyverse)\n\nWhat we can learn from the code chunk above:\n\np_load function pf pacman package is used to install and load sf and tidyverse pacages into R environment."
  },
  {
    "objectID": "Hands_on_Ex/Hands_on_Ex01.html#importing-geospatial-data",
    "href": "Hands_on_Ex/Hands_on_Ex01.html#importing-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling",
    "section": "1.4 Importing Geospatial Data",
    "text": "1.4 Importing Geospatial Data\nIn this section, you will learn how to import the following geospatial data into R by using st_read() of sf package:\n\nMP14_SUBZONE_WEB_PL, a polygon feature layer in ESRI shapefile format,\nCyclingPath, a line feature layer in ESRI shapefile format, and\nPreSchool, a point feature layer in kml file format.\n\n\n1.4.1 Importing polygon feature data in shapefile format\nThe code chunk below uses st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a polygon feature data frame. Note that when the input geospatial data is in shapefile format, two arguments will be used, namely: dsn to define the data path and layer to provide the shapefile name. Also note that no extension such as .shp, .dbf, .prj and .shx are needed.\n\nmpsz = st_read(dsn = \"data\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\G2wongkelly\\IS415_GAA\\Hands_on_Ex\\data' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n1.4.2 Importing polyline feature data in shapefile form\nThe code chunk below uses st_read() function of sf package to import CyclingPath shapefile into R as line feature data frame.\n\ncyclingpath = st_read(dsn = \"data\", \n                         layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `C:\\G2wongkelly\\IS415_GAA\\Hands_on_Ex\\data' using driver `ESRI Shapefile'\nSimple feature collection with 2248 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\nThe message above reveals that there are a total of 1625 features and 2 fields in cyclingpath linestring feature data frame and it is in svy21 projected coordinates system too.\n\n\n1.4.3 Importing GIS data in kml format\nThe pre-schools-location-kml is in kml format. The code chunk below will be used to import the kml into R. Notice that in the code chunk below, the complete path and the kml file extension were provided.\n\npreschool = st_read(\"data/preschools-location.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\G2wongkelly\\IS415_GAA\\Hands_on_Ex\\data\\preschools-location.kml' \n  using driver `KML'\nSimple feature collection with 1925 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThe message above reveals that preschool is a point feature data frame. There are a total of 1359 features and 2 fields. Different from the previous two simple feature data frame, preschool is in wgs84 coordinates system."
  },
  {
    "objectID": "Hands_on_Ex/Hands_on_Ex01.html#checking-the-content-of-a-simple-feature-data-frame",
    "href": "Hands_on_Ex/Hands_on_Ex01.html#checking-the-content-of-a-simple-feature-data-frame",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling",
    "section": "1.5 Checking the Content of A Simple Feature Data Frame",
    "text": "1.5 Checking the Content of A Simple Feature Data Frame\nIn this sub-section, you will learn different ways to retrieve information related to the content of a simple feature data frame.\n\n1.5.1 Working with st_geometry()\nThe column in the sf data.frame that contains the geometries is a list, of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses st_geometry() as shown in the code chunk below.\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\nNotice that the print only displays basic information of the feature class such as type of geometry, the geographic extent of the features and the coordinate system of the data.\n\n\n1.5.2 Working with glimpse()\nBeside the basic feature information, we also would like to learn more about the associated attribute information in the data frame. This is the time you will find glimpse() of dplyr. very handy as shown in the code chunk below.\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area <dbl> 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nglimpse() report reveals the data type of each fields. For example FMEL-UPD_D field is in date data type and X_ADDR, Y_ADDR, SHAPE_L and SHAPE_AREA fields are all in double-precision values.\n\n\n1.5.3 Working with head()\nSometimes we would like to reveal complete information of a feature object, this is the job of head() of Base R\n\nhead(mpsz, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30..."
  },
  {
    "objectID": "Hands_on_Ex/Hands_on_Ex01.html#plotting-the-geospatial-data",
    "href": "Hands_on_Ex/Hands_on_Ex01.html#plotting-the-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling",
    "section": "1.6 Plotting the Geospatial Data",
    "text": "1.6 Plotting the Geospatial Data\nIn geospatial data science, by looking at the feature information is not enough. We are also interested to visualise the geospatial features. This is the time you will find plot() of R Graphic comes in very handy as shown in the code chunk below.\n\nplot(mpsz)\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum as shown above. We can, however, choose to plot only the geometry by using the code chunk below.\n\nplot(st_geometry(mpsz))\n\n\n\n\nAlternatively, we can also choose the plot the sf object by using a specific attribute as shown in the code chunk below.\n\nplot(mpsz[\"PLN_AREA_N\"])"
  },
  {
    "objectID": "Hands_on_Ex/Hands_on_Ex01.html#working-with-projection",
    "href": "Hands_on_Ex/Hands_on_Ex01.html#working-with-projection",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling",
    "section": "1.7 Working with Projection",
    "text": "1.7 Working with Projection\nMap projection is an important property of a geospatial data. In order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system.\nIn this section, you will learn how to project a simple feature data frame from one coordinate system to another coordinate system. The technical term of this process is called projection transformation.\n\n1.7.1 Assigning EPSG code to a simple feature data frame\nOne of the common issue that can happen during importing geospatial data into R is that the coordinate system of the source data was either missing (such as due to missing .proj for ESRI shapefile) or wrongly assigned during the importing process.\nThis is an example the coordinate system of mpsz simple feature data frame by using st_crs() of sf package as shown in the code chunk below.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAlthough mpsz data frame is projected in svy21 but when we read until the end of the print, it indicates that the EPSG is 9001. This is a wrong EPSG code because the correct EPSG code for svy21 should be 3414.\nIn order to assign the correct EPSG code to mpsz data frame, st_set_crs() of sf package is used as shown in the code chunk below.\n\nmpsz3414 <- st_set_crs(mpsz, 3414)\n\nWarning: st_crs<- : replacing crs does not reproject data; use st_transform for\nthat\n\n\nNow, let us check the CSR again by using the code chunk below.\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nNotice that the EPSG code is 3414 now.\n\n\n1.7.2 Transforming the projection of preschool from wgs84 to svy21.\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\nLet us take preschool simple feature data frame as an example. The print below reveals that it is in wgs84 coordinate system.\nGeometry set for 1359 features Geometry type: POINT Dimension: XYZ Bounding box: xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134 z_range: zmin: 0 zmax: 0 Geodetic CRS: WGS 84 First 5 geometries:\nThis is a scenario that st_set_crs() is not appropriate and st_transform() of sf package should be used. This is because we need to reproject preschool from one coordinate system to another coordinate system mathemetically.\nLet us perform the projection transformation by using the code chunk below.\n\npreschool3414 <- st_transform(preschool, \n                              crs = 3414)\n\nNext, let us display the content of preschool3414 sf data frame as shown below.\nGeometry set for 1359 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11203.01 ymin: 25667.6 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\nNotice that it is in svy21 projected coordinate system now. Furthermore, if you refer to Bounding box:, the values are greater than 0-360 range of decimal degree commonly used by most of the geographic coordinate systems."
  },
  {
    "objectID": "Hands_on_Ex/Hands_on_Ex01.html#importing-and-converting-an-aspatial-data",
    "href": "Hands_on_Ex/Hands_on_Ex01.html#importing-and-converting-an-aspatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling",
    "section": "1.8 Importing and Converting An Aspatial Data",
    "text": "1.8 Importing and Converting An Aspatial Data\nIn practice, it is not unusual that we will come across data such as listing of Inside Airbnb. We call this kind of data aspatial data. This is because it is not a geospatial data but among the data fields, there are two fields that capture the x- and y-coordinates of the data points.\nIn this section, you will learn how to import an aspatial data into R environment and save it as a tibble data frame. Next, you will convert it into a simple feature data frame.\nFor the purpose of this exercise, the listings.csv data downloaded from AirBnb will be used.\n\n1.8.1 Importing the aspatial data\nSince listings data set is in csv file format, we will use read_csv() of readr package to import listing.csv as shown the code chunk below. The output R object is called listings and it is a tibble data frame.\n\nlistings <- read_csv(\"data/listings.csv\")\n\nRows: 3037 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (6): name, host_name, neighbourhood_group, neighbourhood, room_type, l...\ndbl  (11): id, host_id, latitude, longitude, price, minimum_nights, number_o...\ndate  (1): last_review\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() of Base R instead of glimpse() is used to do the job.\n\nlist(listings) \n\n[[1]]\n# A tibble: 3,037 × 18\n       id name     host_id host_…¹ neigh…² neigh…³ latit…⁴ longi…⁵ room_…⁶ price\n    <dbl> <chr>      <dbl> <chr>   <chr>   <chr>     <dbl>   <dbl> <chr>   <dbl>\n 1  71609 Ensuite…  367042 Belinda East R… Tampin…    1.35    104. Privat… 46437\n 2  71896 B&B  Ro…  367042 Belinda East R… Tampin…    1.35    104. Privat…    81\n 3  71903 Room 2-…  367042 Belinda East R… Tampin…    1.35    104. Privat…    81\n 4 275343 Amazing… 1439258 Kay     Centra… Bukit …    1.29    104. Privat…    52\n 5 275344 15 mins… 1439258 Kay     Centra… Bukit …    1.29    104. Privat…    49\n 6 289234 Booking…  367042 Belinda East R… Tampin…    1.34    104. Privat…   175\n 7 294281 5 mins … 1521514 Elizab… Centra… Newton     1.31    104. Privat…    79\n 8 324945 Cozy Bl… 1439258 Kay     Centra… Bukit …    1.29    104. Privat…    49\n 9 330089 Cozy Bl… 1439258 Kay     Centra… Bukit …    1.29    104. Privat…    55\n10 330095 10 mins… 1439258 Kay     Centra… Bukit …    1.29    104. Privat…    55\n# … with 3,027 more rows, 8 more variables: minimum_nights <dbl>,\n#   number_of_reviews <dbl>, last_review <date>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <dbl>, availability_365 <dbl>,\n#   number_of_reviews_ltm <dbl>, license <chr>, and abbreviated variable names\n#   ¹​host_name, ²​neighbourhood_group, ³​neighbourhood, ⁴​latitude, ⁵​longitude,\n#   ⁶​room_type\n\n\nThe output reveals that listing tibble data frame consists of 4252 rows and 16 columns. Two useful fields we are going to use in the next phase are latitude and longitude. Note that they are in decimal degree format. As a best guess, we will assume that the data is in wgs84 Geographic Coordinate System.\n\n\n1.8.2 Creating a simple feature data frame from an aspatial data frame\nThe code chunk below converts listing data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nlistings_sf <- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n\nThings to learn from the arguments above:\n\ncoords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\ncrs argument requires you to provide the coordinates system in epsg format. EPSG: 4326 is wgs84 Geographic Coordinate System and EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by referring to epsg.io.\n%>% is used to nest st_transform() to transform the newly created simple feature data frame into svy21 projected coordinates system.\n\nLet us examine the content of this newly created simple feature data frame.\n\nglimpse(listings_sf)\n\nRows: 3,037\nColumns: 17\n$ id                             <dbl> 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           <chr> \"Ensuite Room (Room 1 & 2) near EXPO\", …\n$ host_id                        <dbl> 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      <chr> \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            <chr> \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  <chr> \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      <chr> \"Private room\", \"Private room\", \"Privat…\n$ price                          <dbl> 46437, 81, 81, 52, 49, 175, 79, 49, 55,…\n$ minimum_nights                 <dbl> 92, 92, 92, 60, 60, 92, 92, 60, 60, 60,…\n$ number_of_reviews              <dbl> 20, 24, 47, 22, 14, 12, 133, 17, 12, 4,…\n$ last_review                    <date> 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              <dbl> 0.15, 0.17, 0.33, 0.19, 0.11, 0.09, 1.0…\n$ calculated_host_listings_count <dbl> 6, 6, 6, 46, 46, 6, 7, 46, 46, 46, 6, 7…\n$ availability_365               <dbl> 242, 242, 305, 273, 281, 242, 365, 274,…\n$ number_of_reviews_ltm          <dbl> 0, 0, 0, 2, 1, 0, 0, 3, 2, 1, 1, 0, 0, …\n$ license                        <chr> NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       <POINT [m]> POINT (41972.5 36390.05), POINT (…\n\n\nTable above shows the content of listing_sf. Notice that a new column called geometry has been added into the data frame. On the other hand, the longitude and latitude columns have been dropped from the data frame."
  },
  {
    "objectID": "Hands_on_Ex/Hands_on_Ex01.html#geoprocessing-with-sf-package",
    "href": "Hands_on_Ex/Hands_on_Ex01.html#geoprocessing-with-sf-package",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling",
    "section": "1.9 Geoprocessing with sf package",
    "text": "1.9 Geoprocessing with sf package\nBesides providing functions to handling (i.e. importing, exporting, assigning projection, transforming projection etc) geospatial data, sf package also offers a wide range of geoprocessing (also known as GIS analysis) functions.\nIn this section, you will learn how to perform two commonly used geoprocessing functions, namely buffering and point in polygon count.\n\n1.9.1 Buffering\nThe scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nThe solution:\nFirstly, st_buffer() of sf package is used to compute the 5-meter buffers around cycling paths\n\nbuffer_cycling <- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\nThis is followed by calculating the area of the buffers as shown in the code chunk below.\n\nbuffer_cycling$AREA <- st_area(buffer_cycling)\n\nLastly, sum() of Base R will be used to derive the total land involved\n\nsum(buffer_cycling$AREA)\n\n1556978 [m^2]\n\n\nMission Accomplished!\n\n\n1.9.2 Point-in-polygon count\nThe scenario:\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nThe solution:\nThe code chunk below performs two operations at one go. Firstly, identify pre-schools located inside each Planning Subzone by using st_intersects(). Next, length() of Base R is used to calculate numbers of pre-schools that fall inside each planning subzone.\n\nmpsz3414$`PreSch Count`<- lengths(st_intersects(mpsz3414, preschool3414))\n\nYou can check the summary statistics of the newly derived PreSch Count field by using summary() as shown in the code chunk below.\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    3.00    5.96    9.00   58.00 \n\n\nTo list the planning subzone with the most number of pre-school, the top_n() of dplyr package is used as shown in the code chunk below.\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           58\n\n\nThe solution:\nFirstly, the code chunk below uses st_area() of sf package to derive the area of each planning subzone.\n\nmpsz3414$Area <- mpsz3414 %>%\n  st_area()\n\nNext, mutate() of dplyr package is used to compute the density by using the code chunk below.\n\nmpsz3414 <- mpsz3414 %>%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)"
  },
  {
    "objectID": "Hands_on_Ex/Hands_on_Ex01.html#explorotary-data-analysis-eda",
    "href": "Hands_on_Ex/Hands_on_Ex01.html#explorotary-data-analysis-eda",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling",
    "section": "1.10 Explorotary Data Analysis (EDA)",
    "text": "1.10 Explorotary Data Analysis (EDA)\nIn practice, many geospatial analytics start with Exploratory Data Analysis. In this section, you will learn how to use appropriate ggplot2 functions to create functional and yet truthful statistical graphs for EDA purposes.\nFirstly, we will plot a histogram to reveal the distribution of PreSch Density. Conventionally, hist() of R Graphics will be used as shown in the code chunk below.\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\nAlthough the syntax is very easy to use however the output is far from meeting publication quality. Furthermore, the function has limited room for further customisation.\nIn the code chunk below, appropriate ggplot2 functions will be used.\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\nThe solution:\n\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "Hands_on_Ex/Hands_on_Ex02/Hands_on_Ex02.html",
    "href": "Hands_on_Ex/Hands_on_Ex02/Hands_on_Ex02.html",
    "title": "Hands-on Exersise 2: Choropleth Mapping with R",
    "section": "",
    "text": "Choropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors. For example, a social scientist may need to use a choropleth map to portray the spatial distribution of aged population of Singapore by Master Plan 2014 Subzone Boundary.\nIn this chapter, you will learn how to plot functional and truthful choropleth maps by using an R package called **tmap** package.\n\n\nIt is advisable for you to read the functional description of each function before using them."
  },
  {
    "objectID": "Hands_on_Ex/Hands_on_Ex02/Hands_on_Ex02.html#getting-started",
    "href": "Hands_on_Ex/Hands_on_Ex02/Hands_on_Ex02.html#getting-started",
    "title": "Hands-on Exersise 2: Choropleth Mapping with R",
    "section": "2.2 Getting Started",
    "text": "2.2 Getting Started\nIn this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\npacman::p_load(sf, tmap, tidyverse)\n\nNotice that, we only need to install tidyverse instead of readr, tidyr and dplyr individually."
  },
  {
    "objectID": "Hands_on_Ex/Hands_on_Ex02/Hands_on_Ex02.html#importing-data-into-r",
    "href": "Hands_on_Ex/Hands_on_Ex02/Hands_on_Ex02.html#importing-data-into-r",
    "title": "Hands-on Exersise 2: Choropleth Mapping with R",
    "section": "2.3 Importing Data into R",
    "text": "2.3 Importing Data into R\n\n2.3.1 The Data\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n2.3.2 Importing Geospatial Data into R\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\G2wongkelly\\IS415_GAA\\Hands_on_Ex\\Hands_on_Ex02\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n2.3.3 Importing Attribute Data into R\nNext, we will import respopagsex2000to2018.csv file into RStudio and save the file into an R dataframe called popagsex.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata <- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nRows: 738492 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, FA\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n2.3.4 Data Preparation\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n2.3.4.1 Data wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\n\n\n\n2.3.4.2 Joining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nℹ Please use a list of either functions or lambdas:\n\n# Simple named list: list(mean = mean, median = median)\n\n# Auto named with `tibble::lst()`: tibble::lst(mean, median)\n\n# Using lambdas list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\n\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nThing to learn from the code chunk above:\n\nleft_join() of dplyr package is used with mpsz simple feature data frame as the left data table is to ensure that the output will be a simple features data frame.\n\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Hands_on_Ex/Hands_on_Ex02/Hands_on_Ex02.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Hands_on_Ex/Hands_on_Ex02/Hands_on_Ex02.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exersise 2: Choropleth Mapping with R",
    "section": "2.4 Choropleth Mapping Geospatial Data Using tmap",
    "text": "2.4 Choropleth Mapping Geospatial Data Using tmap\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n2.4.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n2.4.2 Creating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nIn the following sub-section, we will share with you tmap functions that used to plot these elements.\n\n2.4.2.1 Drawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n2.4.2.2 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n2.4.2.3 Drawing a choropleth map using tm_fill() and *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n2.4.3 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n2.4.3.1 Plotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\n\n\n2.4.3.2 Plotting choropleth map with custome break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6540  0.7063  0.7712  0.7657 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\nWarning: Values have found that are higher than the highest break\n\n\n\n\n\n\n\n\n2.4.4 Colour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n2.4.4.1 Using ColourBrewer palette\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the choropleth map is shaded in green.\nTo reverse the colour shading, add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nNotice that the colour scheme has been reversed.\n\n\n\n2.4.5 Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n2.4.5.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n2.4.5.2 Map style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\ntmap style set to \"classic\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"watercolor\" \n\n\n\n\n\n\n\n2.4.5.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\ntmap_style(\"white\")\n\ntmap style set to \"white\"\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\n2.4.6 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n2.4.6.1 By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\ntmap style set to \"white\"\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n2.4.6.2 By defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\nWarning: The argument drop.shapes has been renamed to drop.units, and is\ntherefore deprecated\n\n\n\n\n\n\n\n2.4.6.3 By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n2.4.7 Mappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\nWarning in pre_process_gt(x, interactive = interactive, orig_crs =\ngm$shape.orig_crs): legend.width controls the width of the legend within a map.\nPlease use legend.outside.size to control the width of the outside legend"
  },
  {
    "objectID": "Take_Home_Ex/Take_Home_Ex01/Take_Home_Ex01.html#geospatial-data-cleaning",
    "href": "Take_Home_Ex/Take_Home_Ex01/Take_Home_Ex01.html#geospatial-data-cleaning",
    "title": "Take-Home Exercise 1: Application of Spatial Point Patterns Analysis to discover geographical distribution of functional & non-functional water points in Osub State, Nigeria",
    "section": "4. Geospatial Data Cleaning",
    "text": "4. Geospatial Data Cleaning"
  },
  {
    "objectID": "Take_Home_Ex/Take_Home_Ex01/Take_Home_Ex01.html#excluding-redundent-fields",
    "href": "Take_Home_Ex/Take_Home_Ex01/Take_Home_Ex01.html#excluding-redundent-fields",
    "title": "Take-Home Exercise 1: Application of Spatial Point Patterns Analysis to discover geographical distribution of functional & non-functional water points in Osub State, Nigeria",
    "section": "4.1 Excluding redundent fields",
    "text": "4.1 Excluding redundent fields"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex05/In_Class_Ex05.html",
    "href": "In_Class_Ex/In_Class_Ex05/In_Class_Ex05.html",
    "title": "In-class Exercise 5",
    "section": "",
    "text": "pacman:: p_load(tidyverse,tmap,sf,sfdep)"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex05/In_Class_Ex05.html#running-code",
    "href": "In_Class_Ex/In_Class_Ex05/In_Class_Ex05.html#running-code",
    "title": "In Class Exercise 5",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed)."
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex05/data/stores.html",
    "href": "In_Class_Ex/In_Class_Ex05/data/stores.html",
    "title": "Kelly's IS415 GAA",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     \n\n\n        0 0     false"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex05/data/study_area.html",
    "href": "In_Class_Ex/In_Class_Ex05/data/study_area.html",
    "title": "Kelly's IS415 GAA",
    "section": "",
    "text": "<!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’>     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex05/In_Class_Ex05.html#importing-data",
    "href": "In_Class_Ex/In_Class_Ex05/In_Class_Ex05.html#importing-data",
    "title": "In-class Exercise 5",
    "section": "Importing Data",
    "text": "Importing Data\n\nstudyArea <- st_read(dsn=\"data\",\n                     layer=\"study_area\") %>%\n  st_transform(crs=3829)\n\nReading layer `study_area' from data source \n  `C:\\G2wongkelly\\IS415_GAA\\In_Class_Ex\\In_Class_Ex05\\data' using driver `ESRI Shapefile'\nSimple feature collection with 7 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 121.4836 ymin: 25.00776 xmax: 121.592 ymax: 25.09288\nGeodetic CRS:  TWD97\n\n\n\nstores <- st_read(dsn=\"data\",\n                     layer=\"stores\") %>%\n  st_transform(crs=3829)\n\nReading layer `stores' from data source \n  `C:\\G2wongkelly\\IS415_GAA\\In_Class_Ex\\In_Class_Ex05\\data' using driver `ESRI Shapefile'\nSimple feature collection with 1409 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 121.4902 ymin: 25.01257 xmax: 121.5874 ymax: 25.08557\nGeodetic CRS:  TWD97"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex05/In_Class_Ex05.html#visualising-the-sf-layers",
    "href": "In_Class_Ex/In_Class_Ex05/In_Class_Ex05.html#visualising-the-sf-layers",
    "title": "In-class Exercise 5",
    "section": "Visualising the sf layers",
    "text": "Visualising the sf layers\n\ntmap_mode(\"view\") \n\ntmap mode set to interactive viewing\n\ntm_shape(studyArea) + \n  tm_polygons() +\ntm_shape(stores)+ \n  tm_dots(col = \"Name\", \n          size = 0.01,\n          border.col = \"black\", \n          border.lwd = 0.5) +\n  tm_view(set.zoom.limits = c(12,16))"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex05/In_Class_Ex05.html#local-colocation-quotients-lclq",
    "href": "In_Class_Ex/In_Class_Ex05/In_Class_Ex05.html#local-colocation-quotients-lclq",
    "title": "In-class Exercise 5",
    "section": "Local Colocation Quotients (LCLQ)",
    "text": "Local Colocation Quotients (LCLQ)\n\nnb <- include_self(\n  st_knn(st_geometry(stores),6))\n\nwt <- st_kernel_weights(nb,\n                     stores,\n                     \"gaussian\",\n                     adaptive = TRUE)\n\n\nFamilyMart <- stores %>%\n  filter(Name == \"Family Mart\")\nA <- FamilyMart$Name\n\nSevenEleven <- stores %>%\n  filter(Name == \"7-Eleven\")\nB <- SevenEleven$Name\n\n\nLCLQ <- local_colocation(A,B,nb,wt,49)\n\n\nLCLQ_stores <- cbind(stores, LCLQ)\n\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(studyArea) + \n  tm_polygons() + \n  tm_shape(LCLQ_stores)+\n    tm_dots(col = \"X7.Eleven\",\n            size = 0.01,\n            border.col = \"black\",\n            border.lwd = 0.5) + \n  tm_view(set.zoom.limits = c(12,16))"
  },
  {
    "objectID": "In_Class_Ex/In_Class_Ex06/In_Class_Ex06.html",
    "href": "In_Class_Ex/In_Class_Ex06/In_Class_Ex06.html",
    "title": "In-class Exercise 6: Spatial Weights: sfdep methods",
    "section": "",
    "text": "Installing and Loading the R packages\n\npacman:: p_load(tidyverse,tmap,sf,sfdep)\n\n\n\nThe Data\nFor the purpose of this in-class exercise, the Hunan data sets will be used. There are two data sets in this use case, they are:\n\nHunan, a geospatial data set in ESRI shapefile format, and\nHunan_2012, an attribute data set in csv format\n\n\n\nImporting geospatial data\n\n#st_read is a sf function\nHunan <- st_read(dsn=\"data/geospatial\",\n                     layer=\"Hunan\")\n\nReading layer `Hunan' from data source \n  `C:\\G2wongkelly\\IS415_GAA\\In_Class_Ex\\In_Class_Ex06\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n#from the package read(r) in tidyverse\nHunan_2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\") \n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCombining both data frame by using left join\nNote that one have geometric column but the other dataset doesnt have.\nTherefore, the left input file should be one with sf dataframe and right input file should be normal csv file.\n\n# there is built in intelligence to identify there is common field between the two data sets however, always check the dataset if the column name and data are similar to join\n#R is case sensitive\nHunan_GDPPC <- left_join(Hunan,Hunan_2012)%>%\n  select(1:4, 7, 15)\n\nJoining with `by = join_by(County)`\n\n#after joining, we just retain column 1-4, 7 and 15 only\n\n\n\nPlotting a choropleth map\n\ntmap_mode(\"plot\") \n\ntmap mode set to plotting\n\ntm_shape(Hunan_GDPPC) + #define spatial data you want\n  tm_fill(\"GDPPC\", \n          style = \"quantile\",\n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by district\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha=0.2)\n\n\n\n\n\n\nIdentify area neighbours\nBefore spatial weight matrix can be derived, the neighbours need to be identified first.\n\n\nContiguity neighbours method # redundant if you have the queen’s method\n\ncn_queen <- Hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry), #create a new field call nb and store the result of st_contiguity \n                            .before=1) #put nb newly created field as the first cloumn\n\nArguments:\nGeometry - an sf or sfc object\nqueen - default true\ninspect cn_queen data set\n#Using the steps you just learned, derive a contiguity neighbour list using Rook’s method\\\n\ncn_rook <- Hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         queen = FALSE,\n         .before = 1)\n\n\n\nComputing contiguity weights\n\nContiguity weights: Queen’s method\n\nwm_q <- Hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb),\n         .before = 1)"
  }
]