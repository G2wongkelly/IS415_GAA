---
title: "Take_Home_Ex03_Updated"
author: "Wong Kelly"
date-modified: "`r Sys.Date()`"

format: html
execute: 
  echo: true
  eval: true
  warning: false
editor: visual
---

# 1. Overview

Housing is an essential component of household wealth worldwide. Buying a housing has always been a major investment for most people. The price of housing is affected by many factors. Some of them are global in nature such as the general economy of a country or inflation rate. Others can be more specific to the properties themselves. These factors can be further divided to structural and locational factors. Structural factors are variables related to the property themselves such as the size, fitting, and tenure of the property. Locational factors are variables related to the neighbourhood of the properties such as proximity to childcare centre, public transport service and shopping centre.

![](images/2201175.webp)

Conventional, housing resale prices predictive models were built by using [**Ordinary Least Square (OLS)**](https://en.wikipedia.org/wiki/Ordinary_least_squares) method. However, this method failed to take into consideration that spatial autocorrelation and spatial heterogeneity exist in geographic data sets such as housing transactions. With the existence of spatial autocorrelation, the OLS estimation of predictive housing resale pricing models could lead to biased, inconsistent, or inefficient results (Anselin 1998). In view of this limitation, **Geographical Weighted Models** were introduced for calibrating predictive model for housing resale prices.

## 1.1 The Task

In this take-home exercise, we are tasked to ***predict HDB resale prices at the sub-market level (i.e.Â HDB 3-room, HDB 4-room and HDB 5-room) for the month of January and February 2023 in Singapore.*** The predictive models must be built by using by using conventional OLS method and GWR methods. ***We are also required to compare the performance of the conventional OLS method versus the geographical weighted methods.***

# 2. Data Acquisition Source

For the purpose of this take-home exercise, [`HDB Resale Flat Prices`](https://data.gov.sg/dataset/resale-flat-prices) provided by Data.gov.sg should be used as the core data set. The study should focus on either three-room, four-room or five-room flat and transaction period should be from 1st January 2021 to 31st December 2022. The test data should be January and February 2023 resale prices.

In addition, we will also include other locational factors such as proximity of HDB to eldercare services, and shopping malls etc for considerations.

***Data Summary Table***

| Type                           | Name                                  | Format   | Source              |
|--------------------------------|---------------------------------------|----------|---------------------|
| Aspatial                       | HDB resale flat prices                | .csv     | data.gov.sg         |
| Geospatial                     | Master plan 2014 subzone web boundary | .shp     | data.gov.sg         |
| Geospatial (Locational factor) | Elder care services                   | .shp     | data.gov.sg         |
| Geospatial (Locational factor) | Hawker centres                        | .geojson | data.gov.sg         |
| Geospatial (Locational factor) | MRT stations                          | .geojson | datamall.lta.gov.sg |
| Geospatial (Locational factor) | Supermarkets                          | .geojson | data.gov.sg         |
| Geospatial (Locational factor) | Student care services                 | .geojson | data.gov.sg         |
| Geospatial (Locational factor) | Bus stops                             | .shp     | data.gov.sg         |
| Geospatial (Locational factor) | Dengue clusters                       | .geojson | data.gov.sg         |
| Geospatial (Locational factor) | National parks                        | .shp     | data.gov.sg         |
| Geospatial (Locational factor) | Kindergartens                         | .shp     | data.gov.sg         |
| Geospatial (Locational factor) | Primary Schools                       | .csv     | data.gov.sg         |

# 3. Getting Started

## 3.1 Installing and Loading the R packages

```{r}
pacman::p_load(olsrr, sf, spdep, sfdep, GWmodel, tmap, tidyverse, gtsummary, SpatialML,rsample,Metrics, jsonlite,httr,rvest,sp)
```

The R packages installed that we will be using for take-home assignment 3 are:

-   **olsrr**: designed for use with ordinary least squares (OLS) regression
-   **sf:** used for importing, managing, and processing geospatial data
-   **spdep:** provides function for spatial data analysis
-   **sfdep:** An interface for 'spdep' to integrate with 'sf' objects and the 'tidyverse'
-   **GWmodel**: used for modeling geostatistical data
-   **tmap:** used for creating thematic maps, such as choropleth and bubble maps
-   **tidyverse:** a collection of packages for data science and data analysis
-   **gtsummary:** used for creating publication-ready summary tables of data
-   **SpatialML:** Implements a spatial extension of the random forest algorithm
-   **rsample**: working with data splits, resampling, and cross-validation
-   **Metrics**: Evaluating the performance of predictive models
-   **jsonlite**: Working with JSON data in R
-   **httr:** Working with HTTP web APIs in R
-   **rvest:** Web scraping and data extraction
-   **sp:** Working with spatial data

# 4. Data Wrangling: Geospatial Data & Aspatial Data

## **4.1 Importing Aspatial Data**

```{r}
resale <- read_csv("data/aspatial/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv")
```

From the results above, we can tell that:

-   The dataset contains 11 columns with 148,000 rows in total.

-   The timeframe of the dataset is from 2017 January to 2023 February up to date (from the review of dataset).

-   The columns that are present in the data are: month, town, flat_type, block, street_name, storey_range, floor_area_sqm, flat_model, lease_commence_date, remaining_lease, resale_price (from the review of dataset).

In this take-home assignment, **I selected HDB 4-room flat resale prices to analyse** during the **transaction period from 1st January 2021 to 31st December 2022 (training data) and from January 2023 to February 2023** . Therefore, we will need to filter and only extract data during this period of time frame.

*Edited\*\** Due to the extremely long processing time to run the models for the period of 2 years training data set, we will only be using 2 months worth of data in our random forest model in the later section from 2022 October to 2022 December.

### 4.1.1 Filter Resale Data

As mentioned in the previous section of 4.1, we are only interested in (1) HDB 4-room flat resale prices during the period of (2) 1st January 2021 to 31st December 2022 (training) and January 2023 to February 2023 (with extra one month). Let's filter them!

```{r}
rs_subset <-  filter(resale,flat_type == "4 ROOM") %>% 
              filter(month >= "2021-01" & month <= "2023-03")
rs_subset
```

From the results above, we can tell that:

-   We have successfully filtered our data based on earlier chosen HDB model flat and transaction period!

-   From January 2021 to March 2023, there are **25,582** for 4-room flat in Singapore.

### 4.1.2 Transform Resale Data

After we have extracted the rows of transactions we are interested in, we will then proceed to use *mutate* function of dplyr package to create new variables (columns) in a data frame by applying some transformations to the existing columns.

What we will need to do is:

-   **address**: concatenation of the block and street_name columns using paste() function of base R package.

-   **remaining_lease_yr & remaining_lease_mnth**: Split the year and months part of the remaining_lease respectively using str_sub() function of stringr package then converting the character to integer using as.integer() function of base R package.

-   After performing mutate function, we will store the new data in **rs_transform.**

```{r}
rs_transform <- rs_subset %>%
  mutate(rs_subset, address = paste(block,street_name)) %>%
  mutate(rs_subset, remaining_lease_yr = as.integer(str_sub(remaining_lease, 0, 2))) %>%
  mutate(rs_subset, remaining_lease_mnth = as.integer(str_sub(remaining_lease, 9, 11)))
```

After we have successfully added the three variables (address, remaining_lease_yr, and remaining_lease_mnth) into a new data named rs_transform, we will see some NA values in the remaining_lease_mnth column. Therefore, we will need to replace those with a value of 0 using *is.na()* function of base R package.

```{r}
rs_transform$remaining_lease_mnth[is.na(rs_transform$remaining_lease_mnth)] <- 0
rs_transform
```

Now, as we scroll to the remaining_lease_mnth column, we noticed all initial "NA" values have been replaced by 0!

Next, we do not want to segregate the remaining lease in years and months columns. Instead, we could convert the remaining_lease_yr to months unit and create a new column call total_remaining_lease for easier analysis later using *mutate* function of dplyr package which contains the summation of the remaining_lease_yr and remaining_lease_mnth using *rowSum()* function of base R package. Here is how we do it!

```{r}
# Multiply remaining_lease_yr column in months unit
rs_transform$remaining_lease_yr <- rs_transform$remaining_lease_yr * 12

# Create a new column: total_remaining_lease to contain the summation of yr and mnth
rs_transform <- rs_transform %>% 
  mutate(rs_transform, total_remaining_lease = rowSums(rs_transform[, c("remaining_lease_yr", "remaining_lease_mnth")])) %>%
  select(month, town, address, block, street_name, flat_type, storey_range, floor_area_sqm, flat_model, 
         lease_commence_date, total_remaining_lease, resale_price)

# Display head of data
head(rs_transform)
```

Upon inspection of the rs_transform, we now only left with one column: total_remaining_lease that contains all the remaining lease in months!

### 4.1.3 Retrieve Postal Codes and Coordinates of Addresses

In this section, we will focus on retrieving the relevant data like postal codes and coordinates of the address which is required to get the proximity to locational factors in the later parts.

Here are the steps to add its longitude and latitude features with OneMapSG API!

***Step 1: Create a list storing unique addresses***

```{r}
add_list <- sort(unique(rs_transform$address))
```

***Step 2: Create function to retrieve coordinates from OneMapSG API***

```{r}
get_coords <- function(add_list){
  
  # Create a data frame to store all retrieved coordinates
  postal_coords <- data.frame()
    
  for (i in add_list){

    r <- GET('https://developers.onemap.sg/commonapi/search?',
           query=list(searchVal=i,
                     returnGeom='Y',
                     getAddrDetails='Y'))
    
    # Send a GET request to OneMap API with address as searchVal,
    # returnGeom as 'Y' to retrieve the coordinates, and getAddrDetails as 'Y' to retrieve the postal code

    
    data <- fromJSON(rawToChar(r$content))
    found <- data$found
    res <- data$results
    
    # Extract the 'found' and 'results' fields from the API reponses
    
    # Create a new data frame for each address
    new_row <- data.frame()
    
    # If single result, append 
    if (found == 1){
      postal <- res$POSTAL 
      lat <- res$LATITUDE
      lng <- res$LONGITUDE
      new_row <- data.frame(address= i, postal = postal, latitude = lat, longitude = lng)
    }
    
    # If multiple results, drop NIL and append top 1
    else if (found > 1){
      # Remove those with NIL as postal
      res_sub <- res[res$POSTAL != "NIL", ]
      
      # Set as NA first if no Postal
      if (nrow(res_sub) == 0) {
          new_row <- data.frame(address= i, postal = NA, latitude = NA, longitude = NA)
      }
      
      else{
        top1 <- head(res_sub, n = 1)
        postal <- top1$POSTAL 
        lat <- top1$LATITUDE
        lng <- top1$LONGITUDE
        new_row <- data.frame(address= i, postal = postal, latitude = lat, longitude = lng)
      }
    }

    else {
      new_row <- data.frame(address= i, postal = NA, latitude = NA, longitude = NA)
    }
    
    # Add the row
    postal_coords <- rbind(postal_coords, new_row)
  }
  return(postal_coords)
}
```

***Step 3: Call get_coords function to retrieve resale coordinates***

Note\* After retrieving the coords for the first time, it will be commented out to reduce the rendering time as the time taken to run and process the code chunk below is about 5 to 6 minutes each time.

```{r}
#coords <- get_coords(add_list)
```

The code chunk below will be used to save the coords data file in csv format for future use.

```{r}

#write_rds(coords,"data/rds/coords.csv")
#coords
```

We will then read the coords.csv data and use it for the following parts.

```{r}
coords <- read_rds("data/rds/coords.csv")
coords
```

### 4.1.4 Combine Resale and Coordinates Data

After we have done retrieving the location coordinates of all the resale HDBs, we need to now combine our resale data (rs_transform) earlier with the coordinates data (coords) using *left_join()* function.

```{r}
rs_coords <- left_join(rs_transform, coords, by = c('address' = 'address'))
```

Great! We have successfully joined the two data sets and now let's write the file to our rds folder!

```{r}
rs_coords_rds <- write_rds(rs_coords, "data/rds/rs_coords.rds")
```

Now, let's read rs_coords RDS file:

```{r}
rs_coords <- read_rds("data/rds/rs_coords.rds")
```

### 4.1.5 Assign and Transform CRS and Check

The coordinate columns (latitude, longitude) are currently in decimal degrees, the projected CRS will be WGS84. We will need to convert it into a spatial data frame with projected coordinates of 3414.

```{r}
rs_coords_sf <- st_as_sf(rs_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

```{r}
st_crs(rs_coords_sf)
```

#### 4.1.5.1 Check for Invalid Geometries

```{r}
length(which(st_is_valid(rs_coords_sf) == FALSE))
```

We have no invalid geometries! Now, let's plot hdb resale points

```{r}
tmap_mode("view")
tm_shape(rs_coords_sf)+
  tm_dots(col="red", size = 0.02)
tmap_mode("plot")
```

# 5. Import Geospatial Locational Factors Data (WITH geographic coordinates)

## 5.1 Import ALL Locational Factors Data Sets

In this section, we will read and process all the locational factors data as they are important in determining the HDB resale prices as we believe good location with good amenities will have a higher resale price and vice versa.

Firstly, we will need to read and check CRS of all locational factors!

```{r}
#geojson files
hawker_sf <- st_read("data/geospatial_locational_GC/hawker-centres/hawker-centres-geojson.geojson")
supermarket_sf <- st_read("data/geospatial_locational_GC/supermarkets/supermarkets-geojson.geojson")
student_sf <- st_read("data/geospatial_locational_GC/student-care-services/student-care-services-geojson.geojson")
dengue_sf <- st_read("data/geospatial_locational_GC/dengue-clusters/dengue-clusters-geojson.geojson")
train_sf <- st_read("data/geospatial_locational_GC/mrtstation/lta-mrt-station-exit-geojson.geojson")


#shp files
elder_sf <- st_read(dsn = "data/geospatial_locational_GC/eldercare-services", layer="ELDERCARE")
bus_sf <- st_read(dsn = "data/geospatial_locational_GC/BusStopLocation", layer="BusStop")
kindergarten_sf <- st_read(dsn = "data/geospatial_locational_GC/kindergartens", layer="KINDERGARTENS")
park_sf <- st_read(dsn = "data/geospatial_locational_GC/nationalparks", layer="NATIONALPARKS")

```

From the results above, we can see that the datasets are all in different CRS:

-   The datasets with WGS84 are:

    -   hawker_sf, supermarket_sf, student_sf, dengue_sf,train_sf

-   The datasets with SVY21 are:

    -   elder_sf, bus_sf, kindergarten_sf,park_sf

## 5.2 Transform all Data to CRS EPSG 3414

::: panel-tabset
Transform Data to EPSG 3414

```{r}
elder_sf <- st_set_crs(elder_sf, 3414)
train_sf <- st_set_crs(train_sf, 3414)
bus_sf <- st_set_crs(bus_sf, 3414)
kindergarten_sf <- st_set_crs(kindergarten_sf, 3414)
park_sf <- st_set_crs(park_sf, 3414)

hawker_sf <- hawker_sf %>%
  st_transform(crs = 3414)
supermarket_sf <- supermarket_sf %>%
  st_transform(crs = 3414)
student_sf <- student_sf %>%
  st_transform(crs = 3414)
dengue_sf <- dengue_sf %>%
  st_transform(crs = 3414)
```

st_crs

```{r}
st_crs(elder_sf)
st_crs(train_sf)
st_crs(bus_sf)
st_crs(kindergarten_sf)
st_crs(hawker_sf)
st_crs(supermarket_sf)
st_crs(student_sf)
st_crs(dengue_sf)
st_crs(park_sf)
```
:::

From the above results, we can see that the EPSG code of all the data has now been assigned correctly and they are all EPSG 3414.

### 5.2.1 Check for Invalid Geometries

Since all the datasets above have been converted to the appropraite EPSG, we should also check for any invalid geometries to avoid any issues when calculating proximity or plot the map.

```{r}
length(which(st_is_valid(elder_sf) == FALSE))
length(which(st_is_valid(train_sf) == FALSE))
length(which(st_is_valid(bus_sf) == FALSE))
length(which(st_is_valid(kindergarten_sf) == FALSE))
length(which(st_is_valid(hawker_sf) == FALSE))
length(which(st_is_valid(supermarket_sf) == FALSE))
length(which(st_is_valid(student_sf) == FALSE))
length(which(st_is_valid(dengue_sf) == FALSE))
length(which(st_is_valid(park_sf) == FALSE))
```

From the results above, we can see that there are no invalid geometries for all the locational factors! That means we can move on to calculate proximity.

## 5.3 Calculate Proximity

### 5.3.1 Create get_prox function to calculate proximity

```{r}
get_prox <- function(df1, df2, varname){
  
  # creates a matrix of distances
  dist_matrix <- st_distance(df1, df2)           
  
  # find the nearest location_factor and create new data frame
  near <- df1 %>% 
    mutate(PROX = apply(dist_matrix, 1, function(x) min(x)) / 1000) 
  
  # rename column name according to input parameter
  names(near)[names(near) == 'PROX'] <- varname

  # Return df
  return(near)
}
```

### 5.3.2 Call get_prox function

```{r}
rs_coords_sf <- get_prox(rs_coords_sf, elder_sf, "PROX_ELDERLYCARE") 
rs_coords_sf <- get_prox(rs_coords_sf, train_sf, "PROX_TRAIN") 
rs_coords_sf <- get_prox(rs_coords_sf, bus_sf, "PROX_BUS") 
rs_coords_sf <- get_prox(rs_coords_sf, kindergarten_sf, "PROX_KINDERGARTEN") 
rs_coords_sf <- get_prox(rs_coords_sf, hawker_sf, "PROX_HAWKER")
rs_coords_sf <- get_prox(rs_coords_sf, supermarket_sf, "PROX_SUPERMARKET")
rs_coords_sf <- get_prox(rs_coords_sf, student_sf, "PROX_STUDENT")
rs_coords_sf <- get_prox(rs_coords_sf, dengue_sf, "PROX_DENGUE")
rs_coords_sf <- get_prox(rs_coords_sf, park_sf, "PROX_PARK")
```

### 5.3.3 Create get_within function to calculate factors that are within the declared distance

```{r}
get_within <- function(df1, df2, threshold_dist, varname){
  
  # creates a matrix of distances
  dist_matrix <- st_distance(df1, df2)   
  
  # count the number of location_factors within threshold_dist and create new data frame
  wdist <- df1 %>% 
    mutate(WITHIN_DT = apply(dist_matrix, 1, function(x) sum(x <= threshold_dist)))
  
  # rename column name according to input parameter
  names(wdist)[names(wdist) == 'WITHIN_DT'] <- varname

  # Return df
  return(wdist)
}

```

### **5.3.4 Call get_within function**

-   ***Kindergartens that are within the distance of 350m***

```{r}
rs_coords_sf <- get_within(rs_coords_sf, kindergarten_sf, 350, "WITHIN_350M_KINDERGARTEN")
```

-   ***Childcare services that are within the distance of 350m***

```{r}
rs_coords_sf <- get_within(rs_coords_sf, student_sf, 350, "WITHIN_350M_CHILDCARE")
```

-   ***Bus stop that are within the distance of 350m***

```{r}
rs_coords_sf <- get_within(rs_coords_sf, bus_sf, 350, "WITHIN_350M_BUS")
```

# 6. Import Geospatial Locational Factors Data (WITHOUT geographic coordinates)

In this section, we will retrieve those locational factors that do not have any geographic coordinates.

## 6.1 CBD

Since we are unable to find a list of Singapore Central Business District (CBD) data list and its corresponding geographic coordinates, we will need to do a search of the latitude and longitude of Downtown Core also known as CBD.

Latitude: 1.287953

Longitude: 103.851784

Then, we can create a dataframe consisting of the latitude and longitude coordinates of the CBD area then transform it to EPSG 3414 (SVY21) format.

***Step 1: Store CBD coordinates in database***

```{r}
name <- c('CBD Area')
latitude= c(1.287953)
longitude= c(103.851784)
cbd_coords <- data.frame(name, latitude, longitude)
```

***Step 2: Assign and transform CRS***

```{r}
cbd_coords_sf <- st_as_sf(cbd_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

```{r}
st_crs(cbd_coords_sf)
```

From the results above, we can see that:

-   Coordinates for CBD area in EPSG 3414 (SVY21) format is c(30055.05, 30040.83)

-   We can now run out get_prox function to calculate the proximity of HDB and CBD area!

***Step 3: Call get_prox function***

```{r}
rs_coords_sf <- get_prox(rs_coords_sf, cbd_coords_sf, "PROX_CBD") 
```

## 6.2 Shopping Malls

Similar to CBD, there are no exisiting datasets that we can download for shopping malls in Singapore with corresponding geographic coordinates. Therefore, we would need to extract the Shopping Mall names from Wikipedia and then get the respective coordinates with our get_coords function before computing the proximity.

***Step 1: Extract shopping malls from Wikipedia***

```{r}
url <- "https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore"
malls_list <- list()

for (i in 2:7){
  malls <- read_html(url) %>%
    html_nodes(xpath = paste('//*[@id="mw-content-text"]/div[1]/div[',as.character(i),']/ul/li',sep="") ) %>%
    html_text()
  malls_list <- append(malls_list, malls)
}
```

***Step 2: Call get_coords function***

```{r}
malls_list_coords <- get_coords(malls_list) %>% 
  rename("mall_name" = "address")
```

***Step 3: Remove invalid shopping mall name***

```{r}
malls_list_coords <- subset(malls_list_coords, mall_name!= "Yew Tee Shopping Centre")

```

***Step 4: Correct invalid mall names that can be found***

```{r}
invalid_malls<- subset(malls_list_coords, is.na(malls_list_coords$postal))
invalid_malls_list <- unique(invalid_malls$mall_name)
corrected_malls <- c("Clarke Quay", "City Gate", "Raffles Holland V", "Knightsbridge", "Mustafa Centre", "GR.ID", "Shaw House",
                     "The Poiz Centre", "Velocity @ Novena Square", "Singapore Post Centre", "PLQ Mall", "KINEX", "The Grandstand")

for (i in 1:length(invalid_malls_list)) {
  malls_list_coords <- malls_list_coords %>% 
    mutate(mall_name = ifelse(as.character(mall_name) == invalid_malls_list[i], corrected_malls[i], as.character(mall_name)))
}
```

***Step 5: Create a list storing unique mall names***

```{r}
malls_list <- sort(unique(malls_list_coords$mall_name))
```

***Step 6: Call get_coords to retrieve coordinates of shopping malls again***

```{r}
malls_coords <- get_coords(malls_list)
```

```{r}
malls_coords[(is.na(malls_coords$postal) | is.na(malls_coords$latitude) | is.na(malls_coords$longitude)), ]

```

***Step 7: Convert data frame into sf object, assign and transform crs***

```{r}
malls_sf <- st_as_sf(malls_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

***Step 8: Call get_prox function***

```{r}
rs_coords_sf <- get_prox(rs_coords_sf, malls_sf, "PROX_MALL") 
```

## 6.3 Primary Schools

***Step 1: Read file in csv***

```{r}
pri_sch <- read_csv("data/geospatial_locational_nonGC/school-directory-and-information/general-information-of-schools.csv")
```

***Step 2: Extract primary schools and relevant columns only***

```{r}
pri_sch <- pri_sch %>%
  filter(mainlevel_code == "PRIMARY") %>%
  select(school_name, address, postal_code, mainlevel_code)
```

***Step 3: Create list storing unique postal codes of primary schools***

```{r}
prisch_list <- sort(unique(pri_sch$postal_code))
```

***Step 4: Call get_coords function to retrieve coordinates of primary schools***

```{r}
prisch_coords <- get_coords(prisch_list)
```

```{r}
prisch_coords[(is.na(prisch_coords$postal) | is.na(prisch_coords$latitude) | is.na(prisch_coords$longitude)), ]

```

***Step 5: Combine coordinates with primary school names***

```{r}
prisch_coords = prisch_coords[c("postal","latitude", "longitude")]
pri_sch <- left_join(pri_sch, prisch_coords, by = c('postal_code' = 'postal'))
```

***Step 6: Convert pri_sch data frame into sf object, assign and transform crs***

```{r}
prisch_sf <- st_as_sf(pri_sch,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

***Step 7: Call get_within function to get the no.of primary schools that are within the threshold of 1km or 1000m.***

```{r}
rs_coords_sf <- get_within(rs_coords_sf, prisch_sf, 1000, "WITHIN_1KM_PRISCH")
```

## 6.4 Good Primary Schools (Top 10)

***Step 1: Extract ranking list of Singapore primary schools from www.salary.sg website***

```{r}
url <- "https://www.salary.sg/2021/best-primary-schools-2021-by-popularity/"

good_pri <- data.frame()

schools <- read_html(url) %>%
  html_nodes(xpath = paste('//*[@id="post-3068"]/div[3]/div/div/ol/li') ) %>%
  html_text() 

for (i in (schools)){
  sch_name <- toupper(gsub(" â .*","",i))
  sch_name <- gsub("\\(PRIMARY SECTION)","",sch_name)
  sch_name <- trimws(sch_name)
  new_row <- data.frame(pri_sch_name=sch_name)
  # Add the row
  good_pri <- rbind(good_pri, new_row)
}

top_good_pri <- head(good_pri, 10)
```

***Step 2: Check for good primary schools in primary school dataframe created earlier***

```{r}
top_good_pri$pri_sch_name[!top_good_pri$pri_sch_name %in% prisch_sf$school_name]
```

***Step 3: Create a list storing unique good primary school names***

```{r}
good_pri_list <- unique(top_good_pri$pri_sch_name)
```

***Step 4: Call get_coords function to retrieve coordinates of good primary schools***

```{r}
goodprisch_coords <- get_coords(good_pri_list)
```

```{r}
goodprisch_coords[(is.na(goodprisch_coords$postal) | is.na(goodprisch_coords$latitude) | is.na(goodprisch_coords$longitude)), ]
```

***Step 5: Replace invalid good primary school names***

```{r}
top_good_pri$pri_sch_name[top_good_pri$pri_sch_name == "CHIJ ST. NICHOLAS GIRLSâ SCHOOL"] <- "CHIJ SAINT NICHOLAS GIRLS' SCHOOL"
top_good_pri$pri_sch_name[top_good_pri$pri_sch_name == "ST. HILDAâS PRIMARY SCHOOL"] <- "SAINT HILDA'S PRIMARY SCHOOL"
```

***Step 6: Create a list of unique good primary school names again***

```{r}
good_pri_list <- unique(top_good_pri$pri_sch_name)
```

***Step 7: Call get_coords function to retrieve coordinates of good primary schools again***

```{r}
goodprisch_coords <- get_coords(good_pri_list)
```

```{r}
goodprisch_coords[(is.na(goodprisch_coords$postal) | is.na(goodprisch_coords$latitude) | is.na(goodprisch_coords$longitude)), ]

```

From the result above, we can see that all the coordinates of the good primary schools have been retrieved successfully.

***Step 8: Convert data frame into sf objects, assign and transform crs***

```{r}
goodpri_sf <- st_as_sf(goodprisch_coords,
                    coords = c("longitude", 
                               "latitude"),
                    crs=4326) %>%
  st_transform(crs = 3414)
```

***Step 9: Call get_prox function***

```{r}
rs_coords_sf <- get_prox(rs_coords_sf, goodpri_sf, "PROX_GOOD_PRISCH")

```

***Step 10: Write to rds file and read***

```{r}
rs_factors_rds <- write_rds(rs_coords_sf, "data/rds/rs_factors.rds")
```

```{r}
rs_factors_rds <- read_rds("data/rds/rs_factors.rds")
rs_factors_rds
```

# 7. Import Geospatial Data for Analysis

***Step 1: Read subzone aspatial data file***

```{r}
mpsz_sf <- st_read(dsn = "data/geospatial", layer="MP14_SUBZONE_WEB_PL")
```

***Step 2: Transform CRS***

```{r}
mpsz_sf <- st_transform(mpsz_sf, 3414)
```

## **7.1 Check for invalid geometries**

```{r}
length(which(st_is_valid(mpsz_sf) == FALSE))
```

```{r}
mpsz_sf <- st_make_valid(mpsz_sf)
length(which(st_is_valid(mpsz_sf) == FALSE))
```

```{r}
rs_sf <- read_rds("data/rds/rs_factors.rds")
```

## 7.2 Extract unique storey_range and sort

```{r}
storeys <- sort(unique(rs_sf$storey_range))
```

***Step 2: Create dataframe storey_range_order to store order of storey_range***

```{r}
storey_order <- 1:length(storeys)
storey_range_order <- data.frame(storeys, storey_order)
```

***Step 3: Combine storey_order with resale dataframe***

```{r}
rs_sf <- left_join(rs_sf, storey_range_order, by=c("storey_range" = "storeys"))
```

***Step 4: Write rs_sf data file into rds folder and read***

```{r}
write_rds(rs_sf,"data/rds/rs_sf.rds")
```

```{r}
rs_sf <- read_rds("data/rds/rs_sf.rds")
```

# 8. Building Predictive Models with Geographical Weighted Random Forest Method

## 8.1 Data Sampling

The entire data rs_sf are split into training and test data sets with the corresponding period that we are interested in this assignment.

```{r}
train_data <- rs_sf %>% filter(month >= "2022-10" & month <= "2022-12")
test_data <- rs_sf %>% filter(month >= "2023-01" & month <= "2023-02")
```

Write into rds folder and read them for later part of the model run and analysis!

```{r}
#| eval: false
write_rds(train_data, "data/rds/train_data.rds")
write_rds(test_data, "data/rds/test_data.rds")
```

```{r}
train_data <- read_rds("data/rds/train_data.rds")
test_data <- read_rds("data/rds/test_data.rds")
```

## 8.2 Build a non-spatial multiple linear regression

```{r}
price_mlr <- lm(resale_price ~ floor_area_sqm +
                  storey_order + total_remaining_lease + PROX_GOOD_PRISCH +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_TRAIN + PROX_PARK + PROX_MALL + PROX_BUS + PROX_KINDERGARTEN +
                  PROX_SUPERMARKET + PROX_STUDENT + PROX_DENGUE + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                data=train_data)
summary(price_mlr)
```

```{r}
write_rds(price_mlr, "data/rds/price_mlr.rds" ) 
```

## 8.3 gwr predictive model

In this section, we will calibrate a model to predict HDB resale price by using geographically weighted regression method of GWmodel package.

### 8.3.1 Converting the sf data.frame to SpatialPointDataFrame

```{r}
train_data_sp <- as_Spatial(train_data)
train_data_sp
```

### **8.3.2 Computing adaptive bandwidth**

```{r}
#| eval: false
bw_adaptive <- bw.gwr(resale_price ~ floor_area_sqm +
                  storey_order + total_remaining_lease + PROX_GOOD_PRISCH +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_TRAIN + PROX_PARK + PROX_MALL + PROX_BUS + PROX_KINDERGARTEN +
                  PROX_SUPERMARKET + PROX_STUDENT + PROX_DENGUE + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                  data=train_data_sp,
                  approach="CV",
                  kernel="gaussian",
                  adaptive=TRUE,
                  longlat=FALSE)
```

Write into rds folder and read them for later part of the model run and analysis!

```{r}
#| eval: false
write_rds(bw_adaptive, "data/rds/bw_adaptive.rds")
```

```{r}
bw_adaptive <- read_rds("data/rds/bw_adaptive.rds")
bw_adaptive
```

### **8.3.3 Constructing the adaptive bandwidth gwr model**

```{r}
#| eval: false
gwr_adaptive <- gwr.basic(formula = resale_price ~ floor_area_sqm +
                  storey_order + total_remaining_lease + PROX_GOOD_PRISCH +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_TRAIN + PROX_PARK + PROX_MALL + PROX_BUS + PROX_KINDERGARTEN +
                  PROX_SUPERMARKET + PROX_STUDENT + PROX_DENGUE + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                  data=train_data_sp,
                          bw=bw_adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE,
                          longlat = FALSE)
```

Write into rds folder and read them for later part of the model run and analysis!

```{r}
#| eval: false
write_rds(gwr_adaptive, "data/rds/gwr_adaptive.rds")
```

```{r}
gwr_adaptive <- read_rds("data/rds/gwr_adaptive.rds")
gwr_adaptive
```

## 8.4 Preparing Coordinates Data

### 8.4.1 Extracting coordinates data

```{r}
coords <- st_coordinates(rs_sf)
coords_train <- st_coordinates(train_data)
coords_test <- st_coordinates(test_data)
```

Before continue, we write all the output into rds for future used!

```{r}
coords_train <- write_rds(coords_train, "data/rds/coords_train.rds" )
coords_test <- write_rds(coords_test, "data/rds/coords_test.rds" )
```

### 8.4.2 Droping geometry field

```{r}
train_data <- train_data %>% 
  st_drop_geometry()

```

## 8.5 Calibrating Random Forest Model

In this section, we will calibrate a model to predict HDB resale price by using random forest function of ranger package.

```{r}
set.seed(1234)
rf <- ranger(formula = resale_price ~ floor_area_sqm +
                  storey_order + total_remaining_lease + PROX_GOOD_PRISCH +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_TRAIN + PROX_PARK + PROX_MALL + PROX_BUS + PROX_KINDERGARTEN +
                  PROX_SUPERMARKET + PROX_STUDENT + PROX_DENGUE + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
             data=train_data)

```

```{r}
print(rf)
```

## 8.6 Calibrating Geographical Random Forest Model

In this section, we will calibrate a model to predict HDB resale price by using grf() of SpatialML package.

### **8.6.1 Find the optimise bandwidth to use**

```{r}
#| eval: false
bwRF_adaptive <- grf.bw(formula = resale_price ~ floor_area_sqm +
                  storey_order + total_remaining_lease + PROX_GOOD_PRISCH +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_TRAIN + PROX_PARK + PROX_MALL + PROX_BUS + PROX_KINDERGARTEN +
                  PROX_SUPERMARKET + PROX_STUDENT + PROX_DENGUE + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH, train_data,
                  kernel = "adaptive",
                  coords=coords_train,
                  step = 10,
                  trees=30)

```

### 8.6.2 Calibrating using training data

```{r}
#| eval: false
set.seed(1234)
gwRF_adaptive <- grf(formula = resale_price ~ floor_area_sqm +
                  storey_order + total_remaining_lease + PROX_GOOD_PRISCH +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_TRAIN + PROX_PARK + PROX_MALL + PROX_BUS + PROX_KINDERGARTEN +
                  PROX_SUPERMARKET + PROX_STUDENT + PROX_DENGUE + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_PRISCH,
                     train_data, 
                     ntree = 30,
                     bw=296,
                     kernel="adaptive",
                     coords=coords_train)

```

Write into rds folder and read them for later part of the model run and analysis!

```{r}
#| eval: false
write_rds(gwRF_adaptive, "data/rds/gwRF_adaptive.rds")
```

```{r}
gwRF_adaptive <- read_rds("data/rds/gwRF_adaptive.rds")
```

### 8.6.3 Predicting by using test data

The code chunk below will be used to combine the test data with its corresponding coordinates data.

```{r}
test_data <- cbind(test_data, coords_test) %>%
  st_drop_geometry()
```

Next, predict.grf() of spatialML package will be used to predict the resale value by using the test data and gwRF_adaptive model calibrated earlier.

```{r}
#| eval: false
gwRF_pred <- predict.grf(gwRF_adaptive, 
                           test_data, 
                           x.var.name="X",
                           y.var.name="Y", 
                           local.w=1,
                           global.w=0)
```

Before moving on, let us save the output into rds file for future use.

```{r}
#| eval: false
GRF_pred <- write_rds(gwRF_pred, "data/rds/GRF_pred.rds")
```

Next, we will convert the predicting output into a data frame. The output of the predict.grf() is a vector of predicted values. It is wiser to convert it into a data frame for further visualisation and analysis

```{r}
GRF_pred <- read_rds("data/rds/GRF_pred.rds")
GRF_pred_df <- as.data.frame(GRF_pred)
```

In the code chunk below, cbind() is used to append the predicted values onto test_data_p

```{r}
test_data_p <- cbind(test_data, GRF_pred_df)
```

```{r}
write_rds(test_data_p, "data/rds/test_data_p.rds")
```

### 8.6.4 Calculating root mean square error

```{r}
rmse(test_data_p$resale_price, 
     test_data_p$GRF_pred)
```

### 8.6.5 Visualising the predicted values

```{r}
ggplot(data = test_data_p,
       aes(x = GRF_pred,
           y = resale_price)) +
  geom_point()
```

# 9. Analysis

The scatterplot generated above reveals a robust association between the resale price of HDB and GRF prediction, indicating a superior predictive model because the proximity of all points are close to the diagonal line suggests a linear relationship.

# **10. References** & Resources used

Here are the list of resources used in take-home exercise 3, as well as their links. Special thanks to Seniors work samples and Prof Kam for all the detailed explanations and clear documentary posted!! :))

<https://anyscript.org/ammr-doc/auto_examples/Mocap/plot_Plug-in-gait_Simple_FullBody_GRFPrediction.html>

<https://data36.com/predictive-analytics-101-part-2/>

<https://grf-labs.github.io/grf/REFERENCE.html>

<https://rdrr.io/cran/SpatialML/man/predict.grf.html>
